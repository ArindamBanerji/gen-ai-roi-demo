# SOC Copilot Demo — v2.5 / v3.0 / v4.0 Design Document

**Version:** 2.0
**Date:** February 18, 2026
**Status:** Draft — Design Phase
**ACCP Integration:** Each version progressively realizes ACCP capabilities (see Section 0)

---

## Executive Summary

v2.0 proved the architecture and added CISO-facing business impact. v2.5 adds **interactivity and domain depth** — the prospect can input their own numbers (ROI Calculator), see decisions learn from outcomes (Feedback Loop), watch the system resolve competing policies, and experience a smarter query layer. v3.0 adds **platform depth** — full reinforcement learning on the decision graph, external context ingestion, process intelligence integration, and compliance-grade audit trails. v3.5 and v4.0 formalize the full ACCP control plane and extend to multi-domain operation.

The organizing principle: **v2.5 makes the demo personally relevant to each prospect. v3.0 makes it architecturally unassailable. v3.5/v4.0 prove the platform thesis.**

Each version progressively realizes more ACCP (Agentic Cognitive Control Plane) capabilities. The demo IS the ACCP reference implementation — not a separate product, but the same architecture shipping incrementally.

---

## SECTION 0: ACCP Capability Map

### What Is ACCP?

The Agentic Cognitive Control Plane defines five structural capabilities for governed, self-improving enterprise AI:

1. **Typed-Intent Bus** — normalizes signals into classified intents with a common schema
2. **Situational Mesh** — scores situations using context, KPIs, and drift signals
3. **Eval Gates** — enforces structural safety checks before any action executes
4. **TRIGGERED_EVOLUTION** — writes verified outcomes back to the context graph
5. **Decision Economics** — tags every action with time, cost, and risk impact

### ACCP Capabilities by Demo Version

| # | ACCP Capability | Version | Feature | Status |
|---|---|---|---|---|
| 1 | Context graph substrate | v1.0 | Neo4j with alerts, users, assets, patterns, decisions | Done |
| 2 | Situation classification (Typed-Intent) | v2.0 | Situation Analyzer — 6 types, factors, classification | Done |
| 3 | Eval gates (structural safety) | v2.0 | 4-check sequential animation, BLOCKED banner | Done |
| 4 | TRIGGERED_EVOLUTION | v2.0 | Pattern confidence updates, variant promotion | Done |
| 5 | Decision economics | v2.0 | Per-alert time/cost/risk, aggregate impact | Done |
| 6 | Loop 1: Situational Mesh (within-decision) | v2.0 | Situation Analyzer classifies, evaluates options | Done |
| 7 | Loop 2: AgentEvolver (across-decision) | v2.0 | Prompt variant tracking, auto-promotion | Done |
| 8 | ROI Calculator (prospect-specific economics) | v2.5 | Interactive sliders, PDF export, CFO projections | Next |
| 9 | Outcome Feedback (completes TRIGGERED_EVOLUTION) | v2.5 | Correct/incorrect buttons, graph weight adjustment | Next |
| 10 | Policy Conflict Resolution (governance) | v2.5 | Conflicting policy detection, resolution, audit | Next |
| 11 | Automated outcome detection | v3.0 | SIEM re-open signals, analyst override tracking | Planned |
| 12 | External context ingestion (CONSUME) | v3.0 | Threat intel feeds, CVE databases into graph | Planned |
| 13 | Evidence Ledger (compliance) | v3.0 | Immutable decision records, board-ready export | Planned |
| 14 | Process intelligence integration | v3.0 | Process variant extraction, bottleneck detection | Planned |
| 15 | Full Situational Mesh (sub-150ms scoring) | v3.5 | Multi-signal fusion, drift detection | Planned |
| 16 | Formal Typed-Intent Bus | v3.5 | Intent schema normalization across alert types | Planned |
| 17 | Second domain copilot | v4.0 | ITSM or S2P copilot sharing context graph | Vision |
| 18 | Control Tower routing | v4.0 | Intent routing to specialist copilots via bus | Vision |

**Progress: v2.0 = 7/18 done (39%). v2.5 = 10/18 (56%). v3.0 = 14/18 (78%). v3.5 = 16/18 (89%). v4.0 = 18/18 (100%).**

---

## Current State (v2.0 — Complete)

| Capability | Where | ACCP Primitive | Status |
|---|---|---|---|
| Two learning loops (Situation Analyzer + AgentEvolver) | Tabs 2, 3 | Situational Mesh + AgentEvolver | Done |
| Decision economics (time/cost/risk per option) | Tab 3 | Decision Economics | Done |
| Operational impact narrative ($4,800/mo savings) | Tab 2 | Decision Economics | Done |
| Business impact banner (847 hrs, $127K) | Tab 4 | Decision Economics | Done |
| Two-loop hero diagram | Tab 4 | — (visualization) | Done |
| Blocking demo (eval gate failure) | Tab 2 | Eval Gates | Done |
| Two alert types (travel login + phishing) | Tab 3 | Typed-Intent (implicit) | Done |

### What's Missing (the v2.5/v3/v4 gaps)

| Gap | Why It Matters | Version | ACCP Primitive |
|---|---|---|---|
| Prospect can't input THEIR numbers | Demo feels generic, not personal | v2.5 | Decision Economics (extended) |
| Decisions don't learn from outcomes | "Cool architecture but does it actually learn?" | v2.5 | TRIGGERED_EVOLUTION (completed) |
| No policy conflict handling | CISOs live with conflicting policies daily | v2.5 | Eval Gates (extended) |
| Queries can't recover from exec mistakes | Execs type vague prompts, get confused | v2.5 | Typed-Intent Bus (early) |
| No external context sources | Graph feels static, not alive | v3.0 | CONSUME pattern |
| No compliance/audit trail | CISOs report to boards quarterly | v3.0 | Evidence Ledger |
| No process intelligence | Can't show workflow optimization | v3.0 | Situational Mesh (enriched) |
| Single-domain (SOC only) | VCs want to see platform, not point solution | v4.0 | Control Tower + Typed-Intent Bus |

---

## SECTION 1: v2.5 — Interactive Demo with Domain Depth

**Theme:** "Make it personal. Make it learn. Make it resolve conflict."
**ACCP theme:** Completes the TRIGGERED_EVOLUTION cycle and extends Decision Economics to prospect-specific inputs.
**Estimated effort:** 8-12 prompts across 2-3 sessions
**Target:** Demoable in partner meetings, prospect-specific

---

### 1A. ROI Calculator

**Priority:** Highest — the single strongest CISO closer for partner meetings.
**ACCP:** Extends Decision Economics from demo-internal metrics to prospect-specific projections.

**The Problem:** The impact banner shows OUR numbers (847 hours, $127K). A CISO watching thinks: "Those are your assumptions. What about MY SOC?"

**The Solution:** An interactive calculator where the prospect inputs their environment, and the demo projects savings using their numbers.

#### UI Design

New component — either a modal triggered from Tab 4's impact banner, or a dedicated fifth tab. Modal is recommended (keeps 4-tab structure, feels like a natural extension of Tab 4).

```
+-----------------------------------------------------------+
|  ROI CALCULATOR -- Your SOC, Your Numbers                 |
+-----------------------------------------------------------+
|                                                           |
|  YOUR ENVIRONMENT                                         |
|  +-----------------------------------------------+       |
|  | Alerts per day:        [____500____] < slider  |       |
|  | SOC analysts:          [_____8_____] < slider  |       |
|  | Avg analyst salary:    [___$85,000__] < input  |       |
|  | Current MTTR (min):    [____18_____] < slider  |       |
|  | Current auto-close %:  [____35%____] < slider  |       |
|  | Avg escalation cost:   [___$150____] < input  |       |
|  +-----------------------------------------------+       |
|                                                           |
|  PROJECTED IMPACT (with SOC Copilot)                     |
|  +-----------------------------------------------+       |
|  |                                               |       |
|  |  Auto-close rate:    35% -> 89%  (+54 pts)    |       |
|  |  MTTR:               18 min -> 4.5 min        |       |
|  |  Alerts auto-handled: 445/day (was 175)       |       |
|  |  Analyst hours freed: 1,240/month             |       |
|  |                                               |       |
|  |  +---------------------------------------+    |       |
|  |  |  ANNUAL SAVINGS: $1.26M               |    |       |
|  |  |  Payback period: 6 weeks              |    |       |
|  |  |  ROI: 8.4x in Year 1                  |    |       |
|  |  +---------------------------------------+    |       |
|  |                                               |       |
|  |  Breakdown:                                   |       |
|  |  - Analyst time recovered: $892K/yr           |       |
|  |  - Reduced escalation cost: $234K/yr          |       |
|  |  - Compliance automation: $134K/yr            |       |
|  +-----------------------------------------------+       |
|                                                           |
|  [Export as PDF]  [Email Summary]                         |
|                                                           |
+-----------------------------------------------------------+
```

#### Calculation Logic

```
# Inputs (from sliders)
alerts_per_day = 500
analysts = 8
salary = 85000
current_mttr = 18
current_auto_close = 0.35
escalation_cost = 150

# Constants (from our demo data)
projected_auto_close = 0.89
projected_mttr = current_mttr * 0.25  # 75% reduction
monthly_alerts = alerts_per_day * 30

# Calculations
new_auto_handled = monthly_alerts * projected_auto_close
old_auto_handled = monthly_alerts * current_auto_close
additional_auto_handled = new_auto_handled - old_auto_handled

analyst_hourly = salary / 2080  # $40.87/hr at $85K
minutes_saved_per_alert = current_mttr - projected_mttr  # 13.5 min
hours_freed_monthly = (additional_auto_handled * minutes_saved_per_alert) / 60

annual_analyst_savings = hours_freed_monthly * 12 * analyst_hourly
annual_escalation_savings = additional_auto_handled * 12 * escalation_cost * 0.15
annual_compliance_savings = analysts * 5000

total_annual = annual_analyst_savings + annual_escalation_savings + annual_compliance_savings
```

#### Implementation Plan

| Prompt | Scope | Files |
|---|---|---|
| 7A | Backend: `/api/roi/calculate` endpoint accepting inputs, returning projections | `backend/app/routers/roi.py` |
| 7B | Frontend: ROI Calculator modal component with sliders and live calculation | `frontend/src/components/ROICalculator.tsx` |
| 7C | Frontend: "Calculate Your ROI" button on Tab 4 + PDF export | `CompoundingTab.tsx` + export utility |

---

### 1B. Outcome Feedback Loop (Simplified Loop 3)

**Priority:** High — answers "does it actually learn from being wrong?"
**ACCP:** Completes the TRIGGERED_EVOLUTION cycle. v2.0 shows evolution from prompt variants; v2.5 shows evolution from decision outcomes.

**The Problem:** The demo shows the system making decisions and shows how it evolves prompt variants. But it never shows what happens when a decision turns out to be WRONG. CISOs immediately ask: "What if it auto-closes something that was actually a real threat?"

**The Solution:** After "Apply Recommendation" in Tab 3, add an "Outcome Feedback" step that lets the demo show:
1. Correct outcome -> graph strengthens (edge weights increase, pattern confidence rises)
2. Incorrect outcome -> graph weakens (confidence drops, pattern flagged, analyst alerted)

#### UI Design (Tab 3 addition)

After the current "Closed Loop Execution" section, add:

```
+-----------------------------------------------------------+
|  OUTCOME FEEDBACK (Loop 3: Learning from Results)         |
+-----------------------------------------------------------+
|                                                           |
|  24 hours later -- was this decision correct?             |
|                                                           |
|  [Confirmed Correct]    [Incorrect -- Real Threat]        |
|                                                           |
|  -- If "Confirmed Correct" clicked: --                    |
|                                                           |
|  Graph Update:                                            |
|  - PAT-TRAVEL-001 confidence: 94% -> 94.3% (+0.3)        |
|  - Edge weight (User->TravelContext): 0.91 -> 0.93       |
|  - Decision added to precedent library (128 total)        |
|  - "This pattern is now 0.3% more trusted"                |
|                                                           |
|  -- If "Incorrect" clicked: --                            |
|                                                           |
|  ALERT: Decision outcome negative                         |
|  - PAT-TRAVEL-001 confidence: 94% -> 88% (-6.0)          |
|  - Threshold review triggered for auto-close actions      |
|  - Analyst notification sent: "Review travel alerts"      |
|  - Next 5 travel alerts will route to Tier 2              |
|  - "System is self-correcting -- it learned from this"    |
|                                                           |
+-----------------------------------------------------------+
```

#### Key Demo Narrative

The CISO asks: "What if it's wrong?"

You click "Incorrect -- Real Threat" and say:

> "Watch what happens. Confidence drops 6 points. The next five similar alerts automatically route to a human. The system didn't just fail -- it learned from the failure. And it told your team why. That's the difference between a static playbook and a learning system."

This is devastatingly effective because it directly addresses the #1 CISO fear (automated system missing a real threat) and turns it into a strength.

#### Backend Design

```python
# New endpoint: POST /api/alert/outcome
# Request: { decision_id: str, outcome: "correct" | "incorrect" }
# Response: {
#   graph_updates: [
#     { node: "PAT-TRAVEL-001", field: "confidence", before: 0.94, after: 0.943 },
#     { edge: "User->TravelContext", field: "weight", before: 0.91, after: 0.93 }
#   ],
#   consequence: "Pattern strengthened" | "Threshold review triggered",
#   next_n_alerts_override: null | { action: "escalate_tier2", count: 5 },
#   narrative: "..."
# }
```

#### Implementation Plan

| Prompt | Scope | Files |
|---|---|---|
| 8A | Backend: `/api/alert/outcome` endpoint with graph update logic + narrative | `backend/app/routers/triage.py` + `backend/app/services/feedback.py` |
| 8B | Frontend: Outcome Feedback panel in Tab 3 with two buttons + result display | `AlertTriageTab.tsx` |

---

### 1C. Policy Conflict Resolution

**Priority:** High — extremely practical for CISOs who deal with conflicting policies daily.
**ACCP:** Extends Eval Gates from pass/fail safety checks to governance-level policy arbitration. This is the foundation for formal governance in v3.0+.

**The Problem:** Real SOC example:
- Policy A: "Auto-close travel alerts with VPN match" (efficiency)
- Policy B: "Escalate all alerts for users with risk score > 0.80" (security)
- John Smith: VP Finance (risk 0.85), traveling to Singapore, VPN matches -> BOTH policies apply and CONFLICT

**The Solution:** The Situation Analyzer detects the conflict, shows both policies, explains the resolution, and logs it for audit.

#### UI Design (Tab 3 -- enhancement to Situation Analyzer)

When a policy conflict exists, add a section between Situation Analysis and Recommendation:

```
+-----------------------------------------------------------+
|  POLICY CONFLICT DETECTED                                  |
+-----------------------------------------------------------+
|                                                           |
|  Two policies apply to this alert:                        |
|                                                           |
|  Policy A: AUTO-CLOSE-TRAVEL          Priority: 3         |
|  "Auto-close travel anomaly alerts    Scope: All users    |
|   when VPN matches travel record"     Status: Active      |
|                                                           |
|  Policy B: ESCALATE-HIGH-RISK         Priority: 1         |
|  "Escalate all alerts for users       Scope: Risk > 0.8   |
|   with risk score above 0.80"         Status: Active      |
|                                                           |
|  +---------------------------------------------------+   |
|  | RESOLUTION: Policy B takes precedence              |   |
|  |                                                    |   |
|  | Reason: Higher priority (1 vs 3) AND               |   |
|  | security-first principle -- when efficiency         |   |
|  | and security conflict, security wins.              |   |
|  |                                                    |   |
|  | Action adjusted: ESCALATE_TIER2 (was: auto-        |   |
|  | close). Analyst will review with full context.     |   |
|  |                                                    |   |
|  | Audit: Conflict logged as CON-2026-0847            |   |
|  +---------------------------------------------------+   |
|                                                           |
+-----------------------------------------------------------+
```

#### Why This Matters

1. CISOs recognize this instantly -- they deal with policy conflicts weekly
2. Audit trail -- every conflict resolution is logged with reasoning (SOC2/NIST requirement)
3. Demonstrates graph reasoning -- policies are graph nodes, conflict detection is graph traversal
4. Differentiator -- no SIEM does this; they just execute the first matching rule

#### Backend Design

```python
# New in policy.py
POLICIES = {
    "POL-AUTO-CLOSE-TRAVEL": {
        "action": "false_positive_close",
        "conditions": {"alert_type": "anomalous_login", "travel_match": True},
        "priority": 3,
        "scope": "all_users"
    },
    "POL-ESCALATE-HIGH-RISK": {
        "action": "escalate_tier2",
        "conditions": {"user_risk_score_gt": 0.80},
        "priority": 1,
        "scope": "high_risk_users"
    }
}

def detect_policy_conflicts(alert, context) -> list[PolicyConflict]:
    """Find all applicable policies and detect conflicts."""
    applicable = [p for p in POLICIES if matches(p, alert, context)]
    conflicts = find_conflicts(applicable)  # actions disagree
    return conflicts

def resolve_conflict(conflict) -> PolicyResolution:
    """Resolve by priority, then security-first principle."""
    winner = min(conflict.policies, key=lambda p: p.priority)
    return PolicyResolution(
        winning_policy=winner,
        reason=f"Higher priority ({winner.priority}) + security-first principle",
        audit_id=generate_audit_id()
    )
```

#### Implementation Plan

| Prompt | Scope | Files |
|---|---|---|
| 9A | Backend: Policy definitions, conflict detection, resolution logic | `backend/app/services/policy.py` |
| 9B | Frontend: Policy Conflict panel in Tab 3 Situation Analyzer | `AlertTriageTab.tsx` |

**Note:** This only fires for ALERT-7823 (John Smith, risk 0.85) -- the phishing alert has no policy conflict. This makes the demo narrative richer: "Same system, different alert, different reasoning path."

---

### 1D. Prompt Hub with Executive Error Recovery

**Priority:** Medium — improves Tab 1 usability and demonstrates governed analytics.
**ACCP:** Early realization of the Typed-Intent Bus — normalizing vague human input into classified, routable intents.

**The Problem:** When an exec types "how are we doing" into Tab 1, the system returns nothing or a poor match. There's no recovery path.

**The Solution:** A Prompt Hub layer between the user query and the metric matching that:
1. Normalizes vague queries to canonical metrics
2. Suggests alternatives when no exact match
3. Remembers successful query->metric mappings
4. Shows "Did you mean...?" for near-misses

#### UI Enhancement (Tab 1)

```
Current: User types -> instant match or nothing

Enhanced:
User types "how's security doing?"
-> No exact match
-> "Did you mean one of these?"
   - "Show MTTR by severity" (Mean Time to Respond)
   - "Show auto-close rate" (Alert Resolution Efficiency)
   - "Show analyst efficiency" (Alerts per Analyst per Day)
-> User clicks one -> result displays
-> Mapping saved: "how's security doing" -> [mttr, auto_close, analyst_efficiency]
-> Next time same query -> shows all three
```

#### Implementation Plan

| Prompt | Scope | Files |
|---|---|---|
| 10A | Backend: Fuzzy matching + suggestion engine + mapping store | `backend/app/services/prompt_hub.py` + `backend/app/routers/soc.py` |
| 10B | Frontend: "Did you mean?" suggestions + saved queries panel | `SOCAnalyticsTab.tsx` |

---

### v2.5 Priority Matrix

| Feature | CISO Impact | VC Impact | Demo Wow | Effort | ACCP Capability | Priority |
|---|---|---|---|---|---|---|
| **ROI Calculator** | Very High | Medium | Very High | 3 prompts | Decision Economics (extended) | **P0** |
| **Outcome Feedback** | Very High | Very High | Very High | 2 prompts | TRIGGERED_EVOLUTION (completed) | **P1** |
| **Policy Conflict** | Very High | Medium | Very High | 2 prompts | Eval Gates (extended) | **P1** |
| **Prompt Hub** | Medium | Low | Medium | 2 prompts | Typed-Intent Bus (early) | **P2** |

**Recommended build order:** ROI Calculator -> Outcome Feedback -> Policy Conflict -> Prompt Hub

---

## SECTION 2: v3.0 — Platform Depth (ACCP Realization: 78%)

**Theme:** "From demo to platform. From one copilot to many. From SOC to enterprise."
**ACCP theme:** Automated outcome detection, external CONSUME sources, Evidence Ledger, process intelligence. The demo becomes a governed, self-improving system.
**Estimated effort:** Multiple sessions, architecture changes required
**Target:** VC pitch deck proof points, enterprise pilot readiness

---

### 2A. Full Reinforcement Learning on Decision Graph

**What v2.5's Outcome Feedback started, v3.0 completes.**
**ACCP:** Automated TRIGGERED_EVOLUTION -- the system detects outcomes without human clicks.

v2.5 adds manual feedback buttons (correct/incorrect). v3.0 adds automated outcome detection:

| Signal | Source | What It Means |
|---|---|---|
| Alert re-opened within 24h | SIEM | Auto-close was wrong |
| No re-open after 7 days | SIEM | Auto-close was correct |
| Analyst overrides decision | SOC workflow | System needs recalibration |
| Escalation resolved as FP | Ticket system | Could have been auto-closed |
| Incident confirmed from alert | IR platform | Alert was real, system was right to escalate |

#### Architecture: Decision Graph as a Learning Surface

```
Current (v2.0):
  Alert -> Decision -> Action -> Done

v2.5:
  Alert -> Decision -> Action -> Manual Feedback (buttons)
                                      |
                                Graph Update

v3.0:
  Alert -> Decision -> Action -> Outcome Observation
                                      |
                                Outcome Signal
                                      |
                          +-----------+-----------+
                          |                       |
                    Positive Signal          Negative Signal
                          |                       |
                    Strengthen Path          Weaken Path
                    - Edge weight +0.02     - Edge weight -0.05
                    - Confidence +0.3%      - Confidence -2%
                    - Add to precedents     - Flag for review
                          |                       |
                          +-----------+-----------+
                                      |
                          Graph State Updated
```

#### Key Technical Decisions

1. Asymmetric learning rate -- negative outcomes have 2-3x the impact of positive outcomes (safety-first)
2. Confidence floor -- patterns can never drop below 50% confidence (prevents oscillation)
3. Human-in-the-loop trigger -- if confidence drops >10% in a week, auto-escalate all matching alerts until analyst reviews
4. Temporal decay -- old outcomes matter less (6-month half-life)

#### What This Enables (VC Narrative)

"Every other AI security vendor starts from zero with each customer. Our graph accumulates intelligence. And not just from rule updates -- from actual outcomes. The same alert that failed six months ago? The system remembers and handles it differently now. That's not a feature. That's a moat."

---

### 2B. External Context Ingestion

**ACCP:** Implements the CONSUME pattern from UCL -- the graph ingests context from multiple external sources.

**The Problem:** The context graph only knows what's in the SIEM and Neo4j seed data. Real SOC decisions depend on context from many sources.

#### Context Sources

| Source | What It Provides | Integration |
|---|---|---|
| Threat Intelligence Feeds (MITRE ATT&CK, CISA) | Known attack patterns, IOCs, TTPs | API polling -> new AttackPattern nodes |
| Email/Slack (via Rowboat Labs-style tools) | Internal context ("we're seeing phishing targeting finance") | NLP extraction -> context annotations |
| Meeting Transcripts | SOC standup notes, IR debrief decisions | Summarization -> decision precedents |
| Vulnerability Scanners (Qualys, Tenable) | Asset risk context | Enrichment -> Asset node properties |
| HR Systems | Role changes, departures, new hires | User context updates -> risk recalculation |
| Travel/Expense (Concur, SAP) | Travel plans, locations | Travel context -> existing pattern support |

#### Demo Enhancement

Tab 1 or Tab 3 could show a "Context Sources" indicator:

```
Context Graph: 247 nodes
  +-- SIEM data: 189 nodes
  +-- Threat Intel: 31 nodes (MITRE ATT&CK)
  +-- Slack alerts: 12 nodes (SOC channel, 24h)
  +-- HR system: 8 nodes (recent role changes)
  +-- Vuln scanner: 7 nodes (critical CVEs)

Last refresh: 3 minutes ago
```

This makes the graph feel alive -- it's not a static database but an actively updated context layer.

#### Architecture

```
External Sources -> Ingestion Layer -> UCL Graph
                        |
                  CONSUME operation
                        |
                  Node creation/update
                        |
                  Freshness tracking
                        |
                  Available to all queries
```

Key principle: The ingestion layer is CONSUME-only. It adds context. It never makes decisions or takes actions.

---

### 2C. CISO Domain Depth — Compliance & Alert Volume

Two specific CISO pain points that v2.0 doesn't address:

#### 2C-1. Compliance & Audit Trail (Evidence Ledger)

**ACCP:** The Evidence Ledger is the audit substrate for the entire ACCP pattern -- every typed intent, every eval gate check, every decision, every outcome.

**The Problem:** CISOs report to boards quarterly. They need to prove: "Our security posture improved. Here's the evidence."

**The Solution:** An Evidence Ledger -- every decision creates an immutable record:

```
Evidence Record: EVD-2026-08472
+-- Alert: ALERT-7823
+-- Decision: FALSE_POSITIVE_CLOSE
+-- Confidence: 92%
+-- Context consulted: 47 nodes
+-- Policies evaluated: POL-AUTO-CLOSE-TRAVEL, POL-ESCALATE-HIGH-RISK
+-- Conflict resolution: POL-ESCALATE-HIGH-RISK (priority)
+-- Eval gate: PASSED (4/4 checks, overall 0.97)
+-- Outcome: Confirmed correct (7 days, no re-open)
+-- Graph impact: PAT-TRAVEL-001 confidence +0.3%
+-- Timestamp: 2026-02-17T14:32:01Z (7-year retention)
```

Demo enhancement: A "Compliance Export" button on Tab 4 that generates a sample evidence report showing how every decision is traceable from alert -> context -> reasoning -> action -> outcome.

Mapping to standards: SOC2 Type II (decision traceability), NIST CSF (continuous monitoring evidence), ISO 27001 (information security management records).

#### 2C-2. Alert Volume Simulation

**The Problem:** The demo processes one alert at a time. CISOs deal with 5,000-50,000 alerts/day.

**The Solution:** An "Alert Volume Simulation" mode on Tab 4 that shows:

```
Simulation: 500 alerts/day over 4 weeks

Week 1: 500 alerts -> 175 auto-handled (35%) -> 325 need humans -> 41 analyst-hours/day
Week 2: 500 alerts -> 290 auto-handled (58%) -> 210 need humans -> 26 analyst-hours/day
Week 3: 500 alerts -> 380 auto-handled (76%) -> 120 need humans -> 15 analyst-hours/day
Week 4: 500 alerts -> 445 auto-handled (89%) -> 55 need humans  -> 7 analyst-hours/day

Analyst capacity freed: 34 hours/day (4.25 FTEs)
```

This directly connects to the ROI Calculator (v2.5) -- the simulation provides the proof that the calculator's projections are realistic.

---

### 2D. Process Intelligence Integration (UCL + Celonis)

**ACCP:** Enriches the Situational Mesh with process dynamics. Agents reason over HOW alerts flow through the SOC, not just WHAT the alerts contain.

**The Big Idea:** Nobody has extracted meaningful process intelligence data from tools like Celonis and made it part of a context graph's semantics.

#### What Process Mining Provides

| Data | What It Shows | Graph Representation |
|---|---|---|
| Process variants | How alerts actually flow through the SOC | Paths through the decision graph |
| Bottleneck analysis | Where alerts get stuck | High-latency edges in the graph |
| Conformance checking | Are analysts following playbooks? | Deviation nodes linked to decisions |
| Rework loops | How often are alerts re-opened? | Cycle detection in the graph |
| Resource utilization | Which analysts handle which alert types? | Analyst->AlertType affinity edges |

#### The Integration Vision

```
Process Mining Layer (Celonis/etc.)
        |
  Extracts: process variants, bottlenecks, deviations
        |
  Transforms to: ProcessVariant nodes, Bottleneck nodes, Deviation nodes
        |
UCL Context Graph
  +-- Existing: Alerts, Users, Assets, Patterns, Decisions
  +-- NEW: ProcessVariant, Bottleneck, Deviation, AnalystAffinity
        |
  Enriched queries:
  "Not just WHAT happened, but HOW it flowed and WHERE it got stuck"
```

#### Demo Example

Tab 4 could show a "Process Flow" view:

```
Travel Login Alerts -- Process Variants

Variant A (82% of cases): Detect -> Auto-classify -> Auto-close -> Verify
  Avg time: 3 seconds | Outcome: 97% correct

Variant B (12% of cases): Detect -> Auto-classify -> Escalate -> Manual Review -> Close
  Avg time: 45 minutes | Bottleneck: Manual Review (38 min wait)
  Insight: "These could be auto-closed -- missing VPN context"

Variant C (6% of cases): Detect -> Auto-classify -> Auto-close -> Re-open -> Investigate
  Avg time: 4.2 hours | Rework loop detected
  Insight: "Auto-close threshold too aggressive for risk > 0.80 users"
```

The copilot uses this to optimize its own process: "I see that Variant B has a 38-minute bottleneck because of missing VPN context. If I add VPN checking to my auto-classify step, 80% of Variant B alerts could follow Variant A's 3-second path."

#### Why This Is Unique

1. Nobody has done this -- process mining and AI agents are separate worlds today
2. UCL makes it possible -- the semantic graph is the natural convergence point
3. It's measurable -- process variant shift from B->A is a concrete, provable improvement
4. CISOs understand it -- they've seen process mining in supply chain; applying it to SOC is novel but intuitive

#### Relationship to the Common Semantic Layer

```
Common Semantic Layer (UCL)
  +-- CONSUME from: SIEM, EDW, Process Mining, Threat Intel, HR
  +-- SERVE to: BI dashboards, ML models, RAG pipelines, Agents
  +-- MUTATE via: Agent decisions, Outcome feedback, Pattern learning
  +-- ACTIVATE through: Closed-loop execution, Playbook triggers
```

Process mining data becomes first-class graph semantics -- not just an analytics overlay, but context that agents reason over.

---

## SECTION 3: v3.5 / v4.0 — Full ACCP Realization (Vision)

### 3A. v3.5: Formal Situational Mesh + Typed-Intent Bus

**ACCP: Capabilities 15-16.** These formalize patterns that exist informally in v2.0-v3.0.

**Full Situational Mesh (v3.5):**
The Situation Analyzer in v2.0 classifies alerts into 6 types. v3.5 formalizes this into a sub-150ms Situational Mesh that:
- Fuses multiple signals (SIEM alert + threat intel + user context + process variant + policy state) into a single scored situation
- Detects KPI drift (auto-close rate dropping? escalation cost rising?) and triggers re-evaluation
- Operates as a real-time scoring layer, not a classification step

**Formal Typed-Intent Bus (v3.5):**
v2.0 implicitly types intents (alert classification = intent type). v3.5 formalizes this with:
- A typed schema for intents (AlertIntent, QueryIntent, PolicyIntent, EscalationIntent)
- Intent normalization -- all signals (SIEM, email, API, user query) normalize to the same intent schema
- Intent routing -- the bus routes intents to the appropriate specialist copilot
- This is the foundation for multi-copilot architecture in v4.0

### 3B. v4.0: Control Tower + Multi-Domain

**ACCP: Capabilities 17-18.** The full platform thesis realized.

**Second Domain Copilot (v4.0):**
Add an ITSM or Source-to-Pay copilot that shares the same context graph:
- Different typed intents (IncidentTicket, PriceVariance, POException)
- Different specialist reasoning (SLA evaluation, vendor risk assessment)
- Same graph substrate, same learning loops, same decision economics
- Demonstrates cross-domain compounding -- a SOC pattern about a user informs the S2P copilot about that user's procurement behavior

**Control Tower Routing (v4.0):**
The Control Tower receives all typed intents from the bus and routes them:
- SOC intents -> Situation Analyzer + AgentEvolver
- ITSM intents -> Triage Copilot + Change Advisor
- S2P intents -> Price Guardian + Invoice Concierge
- Cross-domain intents -> multiple copilots with shared context

**Domain Extension Table:**

| Domain | Typed Intent | Specialist Copilots | Graph Entities | Decision Economics |
|---|---|---|---|---|
| SOC (demo) | Alert classifications | Situation Analyzer, AgentEvolver | Alerts, Users, Patterns, Decisions | $127/alert saved |
| Source-to-Pay (ARC) | Price variances, PO exceptions | Price Guardian, Invoice Concierge | Vendors, Contracts, Invoices, Policies | $39M/yr COGS savings |
| ITSM | Incident tickets | Triage Copilot, Change Advisor | Services, Dependencies, SLAs | MTTR x hourly cost |
| AML | Transaction alerts | Risk Scorer, SAR Generator | Entities, Transactions, Patterns | FP cost avoidance |

### 3C. Process Intelligence + Semantic Layer Convergence (Vision)

**This is the long-term differentiator for VC conversations.**

The thesis: The enterprise semantic layer and process intelligence are converging. Today they're separate:
- Semantic layer (dbt, Atlan, Alation) = data definitions
- Process mining (Celonis, Microsoft) = workflow analysis
- AI agents (us) = decision automation

Nobody has connected all three. The UCL does.

```
Current Market (Disconnected):

  Semantic Layer <-> BI Dashboards
  Process Mining <-> Process Optimization
  AI Agents      <-> Task Automation

UCL Vision (Connected):

  Semantic Layer --+
  Process Mining ---+--> UCL Context Graph --> Agents that reason over
  AI Outcomes     --+                         BOTH data semantics AND
                                              process dynamics
```

The competitive moat:
1. Data + Process + Decisions in one graph creates network effects no single vendor can replicate
2. Cross-domain compounding -- SOC process patterns inform supply chain patterns and vice versa
3. Temporal depth -- the graph accumulates months of process intelligence
4. Standards alignment -- maps to OCSF, BPMN, and semantic web standards

> VC Pitch Line: "Everyone's building AI agents. We're building the substrate they reason over. When process intelligence, data semantics, and decision outcomes converge in one graph, you get agents that don't just decide -- they understand WHY the process works the way it does. That's not an agent. That's institutional intelligence."

---

## Implementation Roadmap

### v2.5 Sequence (Next 2-3 Sessions)

```
Session A: ROI Calculator
  Prompt 7A: Backend -- /api/roi/calculate endpoint
  Prompt 7B: Frontend -- ROI Calculator modal with sliders
  Prompt 7C: Tab 4 integration + PDF export button
  -> Commit, test, push

Session B: Outcome Feedback + Policy Conflict
  Prompt 8A: Backend -- /api/alert/outcome endpoint + feedback service
  Prompt 8B: Frontend -- Outcome Feedback panel in Tab 3
  Prompt 9A: Backend -- Policy service with conflict detection
  Prompt 9B: Frontend -- Policy Conflict panel in Tab 3
  -> Commit, test, push

Session C (optional): Prompt Hub + Polish
  Prompt 10A: Backend -- Fuzzy matching + suggestion engine
  Prompt 10B: Frontend -- "Did you mean?" in Tab 1
  -> Commit, test, push
  -> Tag v2.5, update docs
```

### v3.0 Sequence (Longer-Term)

```
Phase 1: Reinforcement Learning on Graph [ACCP: Automated TRIGGERED_EVOLUTION]
  - Automated outcome detection
  - Asymmetric learning rates
  - Confidence management
  - Temporal decay

Phase 2: External Context Ingestion [ACCP: CONSUME pattern]
  - Threat intel feed integration
  - Freshness tracking
  - Context source indicators

Phase 3: Compliance & Evidence Ledger [ACCP: Audit substrate]
  - Immutable decision records
  - Compliance report export
  - Standards mapping (SOC2, NIST, ISO)

Phase 4: Process Intelligence Integration [ACCP: Enriched Situational Mesh]
  - Process variant extraction
  - Bottleneck detection
  - Graph node creation from process data
  - Agent self-optimization from process insights
```

### v3.5 Sequence (Future)

```
Phase 5: Formal Situational Mesh [ACCP: Sub-150ms scoring]
  - Multi-signal fusion
  - KPI drift detection
  - Real-time scoring layer

Phase 6: Formal Typed-Intent Bus [ACCP: Intent normalization]
  - Intent schema definition
  - Signal normalization
  - Intent routing framework
```

### v4.0 Sequence (Vision)

```
Phase 7: Second Domain Copilot [ACCP: Multi-domain]
  - ITSM or S2P copilot
  - Shared graph substrate
  - Cross-domain compounding

Phase 8: Control Tower [ACCP: Full control plane]
  - Intent routing to specialist copilots
  - Cross-copilot coordination
  - Enterprise-scale orchestration
```

---

## Appendix: Key Soundbites by Audience

### For CISOs

| Feature | Soundbite |
|---|---|
| ROI Calculator | "Plug in your numbers. See your savings. Take this to your CFO." |
| Outcome Feedback | "When it's wrong, it learns. When it's right, it gets more confident. Your playbooks don't do that." |
| Policy Conflict | "You have conflicting policies right now. You just don't know it. We find them, resolve them, and audit the resolution." |
| Evidence Ledger | "Board meeting in two weeks? Here's every decision, every outcome, every improvement -- automatically generated." |
| Process Intelligence | "We don't just automate your SOC. We show you WHY alerts take 45 minutes when they should take 3 seconds." |

### For VCs

| Feature | Soundbite |
|---|---|
| ACCP Progress | "Seven of eighteen ACCP capabilities are working in the demo today. v2.5 gets to ten. v3.0 gets to fourteen. This isn't a pitch deck -- it's a roadmap with running code." |
| Decision Graph Learning | "Every decision makes the graph smarter. Every outcome validates or corrects. Competitors start at zero. We start at 10,000 validated decisions." |
| UCL + Process Intelligence | "Everyone's building agents. We're building the substrate -- data semantics, process dynamics, and decision outcomes in one graph. That's institutional intelligence." |
| Multi-Domain Compounding | "SOC copilot learns a pattern. Finance copilot inherits the reasoning structure. Same graph, compounding across domains. That's a platform, not a product." |
| Convergence Thesis | "Semantic layers, process mining, and AI agents are converging. We're the wiring between them. That's not a feature gap -- it's an architectural gap." |

---

## Appendix: Version History

| Version | Date | Changes |
|---|---|---|
| 1.0 | Feb 17, 2026 | Initial design document (v2.5/v3.0) |
| 2.0 | Feb 18, 2026 | Added ACCP capability map (Section 0), ACCP tags on every feature, extended to v3.5/v4.0, added ACCP progress percentages, updated VC soundbites |

---

*SOC Copilot Demo Design Document v2.0 | February 18, 2026*
*Status: Draft -- Design Phase*
*Next action: Session A (ROI Calculator) when ready to build*
