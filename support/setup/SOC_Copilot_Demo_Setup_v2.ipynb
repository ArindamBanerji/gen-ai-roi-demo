{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "SOC_Copilot_Demo_Setup_v2.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SOC Copilot Demo: Infrastructure Setup v2\n",
        "\n",
        "This notebook creates all the infrastructure for the SOC Copilot demo:\n",
        "- **BigQuery**: Dataset and tables for SOC metrics\n",
        "- **Firestore**: Collections with seed data (alerts, assets, users, playbooks)\n",
        "- **Neo4j Aura**: Security graph with ~46 nodes and ~61 relationships\n",
        "- **TRIGGERED_EVOLUTION**: 3 relationships (THE KEY DIFFERENTIATOR)\n",
        "- **SUBJECT_TO & MATCHES**: Required relationships for demo flow\n",
        "\n",
        "## v2 Updates\n",
        "- \u2705 **Colab Secrets support** - Keep passwords out of code!\n",
        "- \u2705 Fixed Gemini model name (`gemini-2.0-flash-001`)\n",
        "- \u2705 Added SUBJECT_TO relationships (Asset \u2192 SLA)\n",
        "- \u2705 Added MATCHES relationships (Alert \u2192 AttackPattern)\n",
        "\n",
        "## Secrets to Configure (\ud83d\udd11 icon in left sidebar)\n",
        "| Secret Name | Example Value |\n",
        "|-------------|---------------|\n",
        "| `GCP_PROJECT_ID` | `gen-ai-roi-demo` |\n",
        "| `NEO4J_URI` | `neo4j+s://a118b6f6.databases.neo4j.io` |\n",
        "| `NEO4J_PASSWORD` | (your password) |\n",
        "| `ANTHROPIC_API_KEY` | `sk-ant-...` |\n",
        "\n",
        "**\u23f1\ufe0f Time to complete:** ~15-20 minutes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This installs the Python packages we need. Run this first.\n# It may take 1-2 minutes.\n\n!pip install --quiet google-cloud-bigquery google-cloud-firestore google-cloud-aiplatform\n!pip install --quiet neo4j python-dotenv\n\nprint(\"\u2713 Packages installed successfully\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Authenticate with Google Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This will open a popup asking you to sign in with your Google account.\n# Sign in with the same account that owns your GCP project.\n\nfrom google.colab import auth\nauth.authenticate_user()\n\nprint(\"\u2713 Authenticated with Google Cloud\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2B: Set Up Colab Secrets (RECOMMENDED)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Colab Secrets keeps your passwords and API keys secure.\n# They're stored encrypted and never visible in your notebook code.\n#\n# HOW TO SET UP:\n# 1. Look at the LEFT SIDEBAR in Colab\n# 2. Click the \ud83d\udd11 KEY icon (below the folder icon)\n# 3. Click \"+ Add new secret\"\n# 4. Add these secrets (name \u2192 value):\n#\n#    GCP_PROJECT_ID     \u2192 your GCP project ID (e.g., \"gen-ai-roi-demo\")\n#    NEO4J_URI          \u2192 your Neo4j URI (e.g., \"neo4j+s://a118b6f6.databases.neo4j.io\")\n#    NEO4J_PASSWORD     \u2192 your Neo4j password\n#    ANTHROPIC_API_KEY  \u2192 your Anthropic API key (sk-ant-...)\n#\n# 5. Toggle \"Notebook access\" ON for each secret\n# 6. The cells below will automatically load these values!\n#\n# If you prefer NOT to use Colab Secrets, just edit the fallback values\n# in Cells 3 and 21 below.\n\nprint(\"\ud83d\udcdd Colab Secrets Setup Checklist:\")\nprint(\"   [ ] GCP_PROJECT_ID\")\nprint(\"   [ ] NEO4J_URI\")\nprint(\"   [ ] NEO4J_PASSWORD\")\nprint(\"   [ ] ANTHROPIC_API_KEY (optional, for later)\")\nprint()\nprint(\"Click the \ud83d\udd11 key icon in the left sidebar to add these secrets.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Set Your Project Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This cell tries to load credentials from Colab Secrets first.\n# If not found, it falls back to the manual values below.\n#\n# TO USE COLAB SECRETS (Recommended):\n# 1. Click the \ud83d\udd11 key icon in the left sidebar\n# 2. Add these secrets:\n#    - GCP_PROJECT_ID: your GCP project ID (e.g., \"gen-ai-roi-demo\")\n#\n# OR: Just edit the fallback values below.\n\nimport os\n\n# Try to load from Colab Secrets first\ntry:\n    from google.colab import userdata\n    PROJECT_ID = userdata.get('GCP_PROJECT_ID')\n    print(\"\u2713 Loaded PROJECT_ID from Colab Secrets\")\nexcept (ImportError, Exception) as e:\n    # Fallback to manual configuration\n    PROJECT_ID = \"soc-copilot-demo\"  # \u2190 CHANGE THIS if not using Colab Secrets\n    print(\"\u2139 Using manual PROJECT_ID configuration\")\n\nREGION = \"us-central1\"\nBIGQUERY_DATASET = \"soc\"\n\n# Set environment variables\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\nos.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n\nprint(f\"\u2713 Configuration set:\")\nprint(f\"  Project ID: {PROJECT_ID}\")\nprint(f\"  Region: {REGION}\")\nprint(f\"  BigQuery Dataset: {BIGQUERY_DATASET}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Enable Required GCP APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This enables all the APIs we need. It may take 2-3 minutes.\n# You'll see output for each API being enabled.\n\nprint(\"Enabling required APIs... (this takes 2-3 minutes)\")\nprint(\"-\" * 50)\n\napis = [\n    \"bigquery.googleapis.com\",\n    \"firestore.googleapis.com\", \n    \"aiplatform.googleapis.com\",\n]\n\nfor api in apis:\n    print(f\"Enabling {api}...\")\n    !gcloud services enable {api} --project={PROJECT_ID}\n\nprint(\"-\" * 50)\nprint(\"\u2713 All APIs enabled\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Initialize Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the Python clients for each service.\n\nfrom google.cloud import bigquery\nfrom google.cloud import firestore\nimport vertexai\n\n# Initialize clients\nbq_client = bigquery.Client(project=PROJECT_ID)\ndb = firestore.Client(project=PROJECT_ID)\nvertexai.init(project=PROJECT_ID, location=REGION)\n\nprint(\"\u2713 Clients initialized:\")\nprint(f\"  BigQuery: {PROJECT_ID}\")\nprint(f\"  Firestore: {PROJECT_ID}\")\nprint(f\"  Vertex AI: {PROJECT_ID} ({REGION})\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Create BigQuery Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the 'soc' dataset in BigQuery.\n\nprint(f\"Creating BigQuery dataset '{BIGQUERY_DATASET}'...\")\n\ndataset_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}\"\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = \"US\"\n\ntry:\n    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n    print(f\"\u2713 Dataset '{BIGQUERY_DATASET}' created (or already exists)\")\nexcept Exception as e:\n    print(f\"Error: {e}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Create BigQuery Tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Creates the tables for SOC metrics, detection rules, and sprawl detection.\n\nprint(\"Creating BigQuery tables...\")\nprint(\"-\" * 50)\n\n# Table 1: Metric Contracts (SOC KPIs)\nmetric_schema = [\n    bigquery.SchemaField(\"metric_id\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"version\", \"INTEGER\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"definition\", \"STRING\"),\n    bigquery.SchemaField(\"formula\", \"STRING\"),\n    bigquery.SchemaField(\"owner_email\", \"STRING\"),\n    bigquery.SchemaField(\"grain\", \"STRING\", mode=\"REPEATED\"),\n    bigquery.SchemaField(\"source_tables\", \"STRING\", mode=\"REPEATED\"),\n    bigquery.SchemaField(\"join_logic\", \"STRING\"),\n    bigquery.SchemaField(\"filters_allowed\", \"STRING\", mode=\"REPEATED\"),\n    bigquery.SchemaField(\"freshness_sla_hours\", \"INTEGER\"),\n    bigquery.SchemaField(\"quality_threshold\", \"FLOAT\"),\n    bigquery.SchemaField(\"status\", \"STRING\"),\n    bigquery.SchemaField(\"deprecated_by\", \"STRING\"),\n    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n    bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\"),\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.metric_contracts\"\ntable = bigquery.Table(table_id, schema=metric_schema)\ntry:\n    table = bq_client.create_table(table, exists_ok=True)\n    print(f\"\u2713 Created table: metric_contracts\")\nexcept Exception as e:\n    print(f\"  Note: {e}\")\n\n# Table 2: SOC Metrics (time-series data)\nsoc_metrics_schema = [\n    bigquery.SchemaField(\"date\", \"DATE\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"severity\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"alert_type\", \"STRING\"),\n    bigquery.SchemaField(\"alerts_received\", \"INTEGER\"),\n    bigquery.SchemaField(\"alerts_auto_closed\", \"INTEGER\"),\n    bigquery.SchemaField(\"alerts_escalated_tier2\", \"INTEGER\"),\n    bigquery.SchemaField(\"alerts_escalated_ir\", \"INTEGER\"),\n    bigquery.SchemaField(\"avg_mttr_minutes\", \"FLOAT\"),\n    bigquery.SchemaField(\"avg_mttd_minutes\", \"FLOAT\"),\n    bigquery.SchemaField(\"fp_count\", \"INTEGER\"),\n    bigquery.SchemaField(\"tp_count\", \"INTEGER\"),\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.soc_metrics\"\ntable = bigquery.Table(table_id, schema=soc_metrics_schema)\ntry:\n    table = bq_client.create_table(table, exists_ok=True)\n    print(f\"\u2713 Created table: soc_metrics\")\nexcept Exception as e:\n    print(f\"  Note: {e}\")\n\n# Table 3: Detection Rules Registry\ndetection_rules_schema = [\n    bigquery.SchemaField(\"rule_id\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"description\", \"STRING\"),\n    bigquery.SchemaField(\"mitre_technique\", \"STRING\"),\n    bigquery.SchemaField(\"severity\", \"STRING\"),\n    bigquery.SchemaField(\"alerts_last_30d\", \"INTEGER\"),\n    bigquery.SchemaField(\"fp_rate\", \"FLOAT\"),\n    bigquery.SchemaField(\"status\", \"STRING\"),\n    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n    bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\"),\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.detection_rules\"\ntable = bigquery.Table(table_id, schema=detection_rules_schema)\ntry:\n    table = bq_client.create_table(table, exists_ok=True)\n    print(f\"\u2713 Created table: detection_rules\")\nexcept Exception as e:\n    print(f\"  Note: {e}\")\n\n# Table 4: Rule Sprawl Registry (for duplicate detection)\nsprawl_schema = [\n    bigquery.SchemaField(\"rule_id\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"similar_rule_id\", \"STRING\", mode=\"REQUIRED\"),\n    bigquery.SchemaField(\"similarity_score\", \"FLOAT\"),\n    bigquery.SchemaField(\"pipeline_count\", \"INTEGER\"),\n    bigquery.SchemaField(\"monthly_alert_impact\", \"INTEGER\"),\n    bigquery.SchemaField(\"estimated_monthly_cost\", \"FLOAT\"),\n    bigquery.SchemaField(\"status\", \"STRING\"),\n    bigquery.SchemaField(\"detected_at\", \"TIMESTAMP\"),\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.rule_sprawl_registry\"\ntable = bigquery.Table(table_id, schema=sprawl_schema)\ntry:\n    table = bq_client.create_table(table, exists_ok=True)\n    print(f\"\u2713 Created table: rule_sprawl_registry\")\nexcept Exception as e:\n    print(f\"  Note: {e}\")\n\nprint(\"-\" * 50)\nprint(\"\u2713 All BigQuery tables created successfully\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Seed BigQuery with SOC Metric Contracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This populates the metric_contracts table with SOC-specific KPIs.\n\nfrom datetime import datetime, timedelta\nimport json\n\nprint(\"Seeding SOC metric contracts...\")\nprint(\"-\" * 50)\n\nmetric_contracts = [\n    {\n        \"metric_id\": \"mttr_by_severity\",\n        \"version\": 2,\n        \"name\": \"MTTR by Severity\",\n        \"definition\": \"Mean Time to Respond - average minutes from alert creation to first analyst action, grouped by severity level.\",\n        \"formula\": \"AVG(TIMESTAMP_DIFF(first_response_time, alert_created_time, MINUTE))\",\n        \"owner_email\": \"soc_analytics@company.com\",\n        \"grain\": [\"severity\", \"week_start\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.analyst_actions\"],\n        \"join_logic\": \"alerts.alert_id = analyst_actions.alert_id\",\n        \"filters_allowed\": [\"severity\", \"alert_type\", \"analyst_id\", \"shift\"],\n        \"freshness_sla_hours\": 1,\n        \"quality_threshold\": 0.98,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-08-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-20T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"mttd_by_source\",\n        \"version\": 1,\n        \"name\": \"MTTD by Source\",\n        \"definition\": \"Mean Time to Detect - average minutes from event occurrence to alert generation, by detection source.\",\n        \"formula\": \"AVG(TIMESTAMP_DIFF(alert_created_time, event_time, MINUTE))\",\n        \"owner_email\": \"soc_analytics@company.com\",\n        \"grain\": [\"source\", \"day\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.raw_events\"],\n        \"join_logic\": \"alerts.event_id = raw_events.event_id\",\n        \"filters_allowed\": [\"source\", \"severity\", \"alert_type\"],\n        \"freshness_sla_hours\": 4,\n        \"quality_threshold\": 0.95,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-09-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-18T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"auto_close_rate\",\n        \"version\": 2,\n        \"name\": \"Auto-Close Rate\",\n        \"definition\": \"Percentage of alerts automatically closed by the SOC Copilot without human intervention.\",\n        \"formula\": \"COUNT(CASE WHEN closed_by = 'copilot' THEN 1 END) / COUNT(*) * 100\",\n        \"owner_email\": \"soc_analytics@company.com\",\n        \"grain\": [\"week\", \"alert_type\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.resolutions\"],\n        \"join_logic\": \"alerts.alert_id = resolutions.alert_id\",\n        \"filters_allowed\": [\"alert_type\", \"severity\", \"pattern_matched\"],\n        \"freshness_sla_hours\": 1,\n        \"quality_threshold\": 0.99,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-10-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-25T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"fp_rate_by_rule\",\n        \"version\": 1,\n        \"name\": \"False Positive Rate by Rule\",\n        \"definition\": \"Percentage of alerts from each detection rule that were confirmed as false positives.\",\n        \"formula\": \"COUNT(CASE WHEN resolution = 'false_positive' THEN 1 END) / COUNT(*) * 100\",\n        \"owner_email\": \"detection_engineering@company.com\",\n        \"grain\": [\"rule_id\", \"week\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.resolutions\"],\n        \"join_logic\": \"alerts.alert_id = resolutions.alert_id\",\n        \"filters_allowed\": [\"rule_id\", \"severity\", \"mitre_technique\"],\n        \"freshness_sla_hours\": 24,\n        \"quality_threshold\": 0.95,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-07-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-15T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"escalation_rate\",\n        \"version\": 1,\n        \"name\": \"Escalation Rate\",\n        \"definition\": \"Percentage of alerts escalated to Tier 2 or Incident Response team.\",\n        \"formula\": \"COUNT(CASE WHEN escalated = true THEN 1 END) / COUNT(*) * 100\",\n        \"owner_email\": \"soc_analytics@company.com\",\n        \"grain\": [\"week\", \"severity\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.escalations\"],\n        \"join_logic\": \"alerts.alert_id = escalations.alert_id\",\n        \"filters_allowed\": [\"severity\", \"alert_type\", \"escalation_reason\"],\n        \"freshness_sla_hours\": 4,\n        \"quality_threshold\": 0.98,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-08-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-10T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"analyst_efficiency\",\n        \"version\": 1,\n        \"name\": \"Analyst Efficiency\",\n        \"definition\": \"Average number of alerts resolved per analyst per shift.\",\n        \"formula\": \"COUNT(resolved_alerts) / COUNT(DISTINCT analyst_id || shift_date)\",\n        \"owner_email\": \"soc_management@company.com\",\n        \"grain\": [\"analyst_id\", \"shift_date\"],\n        \"source_tables\": [\"soc.resolutions\", \"soc.analyst_shifts\"],\n        \"join_logic\": \"resolutions.analyst_id = analyst_shifts.analyst_id\",\n        \"filters_allowed\": [\"analyst_id\", \"shift\", \"alert_type\"],\n        \"freshness_sla_hours\": 24,\n        \"quality_threshold\": 0.95,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-09-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-20T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"sla_compliance\",\n        \"version\": 2,\n        \"name\": \"SLA Compliance Rate\",\n        \"definition\": \"Percentage of alerts responded to within SLA timeframes by severity.\",\n        \"formula\": \"COUNT(CASE WHEN response_time <= sla_target THEN 1 END) / COUNT(*) * 100\",\n        \"owner_email\": \"soc_management@company.com\",\n        \"grain\": [\"severity\", \"week\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.sla_targets\", \"soc.resolutions\"],\n        \"join_logic\": \"alerts.severity = sla_targets.severity AND alerts.alert_id = resolutions.alert_id\",\n        \"filters_allowed\": [\"severity\", \"shift\", \"alert_type\"],\n        \"freshness_sla_hours\": 4,\n        \"quality_threshold\": 0.99,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-06-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-22T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"pattern_match_rate\",\n        \"version\": 1,\n        \"name\": \"Pattern Match Rate\",\n        \"definition\": \"Percentage of alerts that matched a known attack pattern.\",\n        \"formula\": \"COUNT(CASE WHEN pattern_id IS NOT NULL THEN 1 END) / COUNT(*) * 100\",\n        \"owner_email\": \"soc_analytics@company.com\",\n        \"grain\": [\"week\", \"alert_type\"],\n        \"source_tables\": [\"soc.alerts\", \"soc.pattern_matches\"],\n        \"join_logic\": \"alerts.alert_id = pattern_matches.alert_id\",\n        \"filters_allowed\": [\"alert_type\", \"pattern_id\"],\n        \"freshness_sla_hours\": 1,\n        \"quality_threshold\": 0.95,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-11-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-18T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"mttr_by_severity_legacy\",\n        \"version\": 1,\n        \"name\": \"MTTR by Severity (Legacy)\",\n        \"definition\": \"Old MTTR calculation using different timestamp fields.\",\n        \"formula\": \"AVG(close_time - open_time)\",\n        \"owner_email\": \"legacy_soc@company.com\",\n        \"grain\": [\"severity\"],\n        \"source_tables\": [\"legacy.tickets\"],\n        \"join_logic\": None,\n        \"filters_allowed\": [\"severity\"],\n        \"freshness_sla_hours\": 48,\n        \"quality_threshold\": 0.85,\n        \"status\": \"deprecated\",\n        \"deprecated_by\": \"mttr_by_severity\",\n        \"created_at\": \"2023-01-15T00:00:00Z\",\n        \"updated_at\": \"2025-06-01T00:00:00Z\",\n    },\n    {\n        \"metric_id\": \"threat_coverage\",\n        \"version\": 1,\n        \"name\": \"MITRE ATT&CK Coverage\",\n        \"definition\": \"Percentage of MITRE ATT&CK techniques covered by active detection rules.\",\n        \"formula\": \"COUNT(DISTINCT covered_techniques) / COUNT(DISTINCT all_techniques) * 100\",\n        \"owner_email\": \"detection_engineering@company.com\",\n        \"grain\": [\"tactic\", \"month\"],\n        \"source_tables\": [\"soc.detection_rules\", \"mitre.techniques\"],\n        \"join_logic\": \"detection_rules.mitre_technique = techniques.technique_id\",\n        \"filters_allowed\": [\"tactic\", \"platform\"],\n        \"freshness_sla_hours\": 168,\n        \"quality_threshold\": 0.90,\n        \"status\": \"active\",\n        \"deprecated_by\": None,\n        \"created_at\": \"2025-05-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-05T00:00:00Z\",\n    },\n]\n\n# Insert into BigQuery\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.metric_contracts\"\n\nfor contract in metric_contracts:\n    contract[\"grain\"] = json.dumps(contract[\"grain\"]) if isinstance(contract[\"grain\"], list) else contract[\"grain\"]\n    contract[\"source_tables\"] = json.dumps(contract[\"source_tables\"]) if isinstance(contract[\"source_tables\"], list) else contract[\"source_tables\"]\n    contract[\"filters_allowed\"] = json.dumps(contract[\"filters_allowed\"]) if isinstance(contract[\"filters_allowed\"], list) else contract[\"filters_allowed\"]\n\nrows_to_insert = []\nfor c in metric_contracts:\n    row = {\n        \"metric_id\": c[\"metric_id\"],\n        \"version\": c[\"version\"],\n        \"name\": c[\"name\"],\n        \"definition\": c[\"definition\"],\n        \"formula\": c[\"formula\"],\n        \"owner_email\": c[\"owner_email\"],\n        \"grain\": json.loads(c[\"grain\"]) if isinstance(c[\"grain\"], str) else c[\"grain\"],\n        \"source_tables\": json.loads(c[\"source_tables\"]) if isinstance(c[\"source_tables\"], str) else c[\"source_tables\"],\n        \"join_logic\": c[\"join_logic\"],\n        \"filters_allowed\": json.loads(c[\"filters_allowed\"]) if isinstance(c[\"filters_allowed\"], str) else c[\"filters_allowed\"],\n        \"freshness_sla_hours\": c[\"freshness_sla_hours\"],\n        \"quality_threshold\": c[\"quality_threshold\"],\n        \"status\": c[\"status\"],\n        \"deprecated_by\": c[\"deprecated_by\"],\n        \"created_at\": c[\"created_at\"],\n        \"updated_at\": c[\"updated_at\"],\n    }\n    rows_to_insert.append(row)\n\nerrors = bq_client.insert_rows_json(table_id, rows_to_insert)\nif errors:\n    print(f\"Errors: {errors}\")\nelse:\n    print(f\"\u2713 Inserted {len(rows_to_insert)} metric contracts\")\n\nprint(\"-\" * 50)\nprint(\"\u2713 SOC metric contracts seeded\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Seed BigQuery with SOC Metrics (Time Series)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates 4 weeks of realistic SOC metrics data.\n\nprint(\"Seeding SOC metrics (4 weeks of data)...\")\nprint(\"-\" * 50)\n\nfrom datetime import datetime, timedelta\nimport random\n\n# Generate 4 weeks of data\nbase_date = datetime(2026, 1, 6)  # Start of first week\nsoc_metrics_data = []\n\nseverities = [\"critical\", \"high\", \"medium\", \"low\"]\nalert_types = [\"anomalous_login\", \"phishing\", \"malware_detection\", \"data_exfiltration\"]\n\n# Week 1: Lower auto-close, higher MTTR (baseline)\n# Week 4: Higher auto-close, lower MTTR (improvement)\nfor week in range(4):\n    week_start = base_date + timedelta(weeks=week)\n    \n    # Calculate improvement factors\n    improvement = week * 0.07  # 7% improvement per week\n    \n    for day_offset in range(7):\n        current_date = week_start + timedelta(days=day_offset)\n        \n        for severity in severities:\n            for alert_type in alert_types:\n                # Base values vary by severity\n                base_alerts = {\"critical\": 15, \"high\": 45, \"medium\": 120, \"low\": 200}[severity]\n                base_auto_close = {\"critical\": 0.3, \"high\": 0.5, \"medium\": 0.7, \"low\": 0.85}[severity]\n                base_mttr = {\"critical\": 8, \"high\": 15, \"medium\": 45, \"low\": 120}[severity]\n                \n                # Add randomness and improvement\n                alerts = int(base_alerts * (0.8 + random.random() * 0.4))\n                auto_close_rate = min(0.95, base_auto_close + improvement + random.random() * 0.05)\n                auto_closed = int(alerts * auto_close_rate)\n                \n                escalated_tier2 = int((alerts - auto_closed) * 0.6)\n                escalated_ir = int((alerts - auto_closed) * 0.3)\n                \n                mttr = max(2, base_mttr * (1 - improvement * 0.5) + random.random() * 5)\n                mttd = max(1, 3 + random.random() * 2)\n                \n                fp_count = int(auto_closed * (0.8 + random.random() * 0.15))\n                tp_count = alerts - fp_count\n                \n                soc_metrics_data.append({\n                    \"date\": current_date.strftime(\"%Y-%m-%d\"),\n                    \"severity\": severity,\n                    \"alert_type\": alert_type,\n                    \"alerts_received\": alerts,\n                    \"alerts_auto_closed\": auto_closed,\n                    \"alerts_escalated_tier2\": escalated_tier2,\n                    \"alerts_escalated_ir\": escalated_ir,\n                    \"avg_mttr_minutes\": round(mttr, 1),\n                    \"avg_mttd_minutes\": round(mttd, 1),\n                    \"fp_count\": fp_count,\n                    \"tp_count\": tp_count,\n                })\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.soc_metrics\"\nerrors = bq_client.insert_rows_json(table_id, soc_metrics_data)\nif errors:\n    print(f\"Errors: {errors[:3]}\")  # Show first 3 errors\nelse:\n    print(f\"\u2713 Inserted {len(soc_metrics_data)} SOC metric rows (4 weeks)\")\n\nprint(\"-\" * 50)\nprint(\"\u2713 SOC metrics seeded\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Seed BigQuery with Detection Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates sample detection rules for the demo.\n\nprint(\"Seeding detection rules registry...\")\nprint(\"-\" * 50)\n\ndetection_rules = [\n    {\n        \"rule_id\": \"anomalous_login_v2\",\n        \"name\": \"Anomalous Login Detection v2\",\n        \"description\": \"Detects logins from unusual locations, times, or devices\",\n        \"mitre_technique\": \"T1078\",\n        \"severity\": \"medium\",\n        \"alerts_last_30d\": 4523,\n        \"fp_rate\": 0.72,\n        \"status\": \"active\",\n        \"created_at\": \"2025-03-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-20T00:00:00Z\",\n    },\n    {\n        \"rule_id\": \"anomalous_login_legacy\",\n        \"name\": \"Anomalous Login (Legacy)\",\n        \"description\": \"Old version - uses IP geolocation only\",\n        \"mitre_technique\": \"T1078\",\n        \"severity\": \"medium\",\n        \"alerts_last_30d\": 2400,\n        \"fp_rate\": 0.89,\n        \"status\": \"deprecated\",\n        \"created_at\": \"2023-06-01T00:00:00Z\",\n        \"updated_at\": \"2025-12-01T00:00:00Z\",\n    },\n    {\n        \"rule_id\": \"phishing_email_v3\",\n        \"name\": \"Phishing Email Detection v3\",\n        \"description\": \"Identifies phishing attempts using URL analysis and sender reputation\",\n        \"mitre_technique\": \"T1566.001\",\n        \"severity\": \"high\",\n        \"alerts_last_30d\": 1876,\n        \"fp_rate\": 0.15,\n        \"status\": \"active\",\n        \"created_at\": \"2025-08-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-18T00:00:00Z\",\n    },\n    {\n        \"rule_id\": \"malware_hash_match\",\n        \"name\": \"Known Malware Hash Detection\",\n        \"description\": \"Matches file hashes against threat intelligence feeds\",\n        \"mitre_technique\": \"T1204\",\n        \"severity\": \"critical\",\n        \"alerts_last_30d\": 234,\n        \"fp_rate\": 0.02,\n        \"status\": \"active\",\n        \"created_at\": \"2024-01-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-25T00:00:00Z\",\n    },\n    {\n        \"rule_id\": \"dlp_sensitive_data\",\n        \"name\": \"Sensitive Data Exfiltration\",\n        \"description\": \"Detects potential data exfiltration based on volume and destination\",\n        \"mitre_technique\": \"T1048\",\n        \"severity\": \"high\",\n        \"alerts_last_30d\": 456,\n        \"fp_rate\": 0.35,\n        \"status\": \"active\",\n        \"created_at\": \"2025-04-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-15T00:00:00Z\",\n    },\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.detection_rules\"\nerrors = bq_client.insert_rows_json(table_id, detection_rules)\nif errors:\n    print(f\"Errors: {errors}\")\nelse:\n    print(f\"\u2713 Inserted {len(detection_rules)} detection rules\")\n\nprint(\"-\" * 50)\nprint(\"\u2713 Detection rules seeded\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Seed BigQuery with Rule Sprawl Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the sprawl detection data showing duplicate rules.\n\nprint(\"Seeding rule sprawl registry...\")\nprint(\"-\" * 50)\n\nrule_sprawl = [\n    {\n        \"rule_id\": \"anomalous_login_legacy\",\n        \"similar_rule_id\": \"anomalous_login_v2\",\n        \"similarity_score\": 0.87,\n        \"pipeline_count\": 3,\n        \"monthly_alert_impact\": 2400,\n        \"estimated_monthly_cost\": 18000.0,\n        \"status\": \"reviewing\",\n        \"detected_at\": \"2026-01-15T00:00:00Z\",\n    },\n]\n\ntable_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.rule_sprawl_registry\"\nerrors = bq_client.insert_rows_json(table_id, rule_sprawl)\nif errors:\n    print(f\"Errors: {errors}\")\nelse:\n    print(f\"\u2713 Inserted {len(rule_sprawl)} sprawl detection entries\")\n\nprint(\"-\" * 50)\nprint(\"\u2713 Rule sprawl data seeded\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Create Firestore Collections - Assets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the assets (devices/systems) for the security demo.\n\nprint(\"Creating Firestore assets collection...\")\nprint(\"-\" * 50)\n\nassets = [\n    {\n        \"asset_id\": \"LAPTOP-JSMITH\",\n        \"hostname\": \"LAPTOP-JSMITH\",\n        \"type\": \"endpoint\",\n        \"os\": \"Windows 11 Enterprise\",\n        \"criticality\": \"high\",\n        \"business_unit\": \"Finance\",\n        \"owner_id\": \"jsmith@company.com\",\n        \"last_seen\": \"2026-01-31T10:30:00Z\",\n    },\n    {\n        \"asset_id\": \"SRV-DB-PROD-01\",\n        \"hostname\": \"SRV-DB-PROD-01\",\n        \"type\": \"server\",\n        \"os\": \"RHEL 8.6\",\n        \"criticality\": \"critical\",\n        \"business_unit\": \"IT\",\n        \"owner_id\": \"db_admin@company.com\",\n        \"last_seen\": \"2026-01-31T10:35:00Z\",\n    },\n    {\n        \"asset_id\": \"LAPTOP-MCHEN\",\n        \"hostname\": \"LAPTOP-MCHEN\",\n        \"type\": \"endpoint\",\n        \"os\": \"macOS Sonoma\",\n        \"criticality\": \"high\",\n        \"business_unit\": \"Engineering\",\n        \"owner_id\": \"mchen@company.com\",\n        \"last_seen\": \"2026-01-31T09:15:00Z\",\n    },\n    {\n        \"asset_id\": \"LAPTOP-AGARCIA\",\n        \"hostname\": \"LAPTOP-AGARCIA\",\n        \"type\": \"endpoint\",\n        \"os\": \"macOS Sonoma\",\n        \"criticality\": \"medium\",\n        \"business_unit\": \"Engineering\",\n        \"owner_id\": \"agarcia@company.com\",\n        \"last_seen\": \"2026-01-31T08:45:00Z\",\n    },\n    {\n        \"asset_id\": \"MAIL-GW-01\",\n        \"hostname\": \"MAIL-GW-01\",\n        \"type\": \"network\",\n        \"os\": \"Proofpoint Appliance\",\n        \"criticality\": \"critical\",\n        \"business_unit\": \"IT\",\n        \"owner_id\": \"mail_admin@company.com\",\n        \"last_seen\": \"2026-01-31T10:40:00Z\",\n    },\n]\n\nfor asset in assets:\n    db.collection(\"assets\").document(asset[\"asset_id\"]).set(asset)\n    print(f\"  \u2713 Created asset: {asset['asset_id']}\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(assets)} assets\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 13: Create Firestore Collections - Users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates user profiles with risk scores.\n\nprint(\"Creating Firestore users collection...\")\nprint(\"-\" * 50)\n\nusers = [\n    {\n        \"user_id\": \"jsmith@company.com\",\n        \"name\": \"John Smith\",\n        \"title\": \"VP Finance\",\n        \"department\": \"Finance\",\n        \"risk_score\": 0.85,\n        \"is_privileged\": True,\n        \"manager\": \"cjohnson@company.com\",\n    },\n    {\n        \"user_id\": \"mchen@company.com\",\n        \"name\": \"Mary Chen\",\n        \"title\": \"Director Engineering\",\n        \"department\": \"Engineering\",\n        \"risk_score\": 0.72,\n        \"is_privileged\": True,\n        \"manager\": \"cjohnson@company.com\",\n    },\n    {\n        \"user_id\": \"agarcia@company.com\",\n        \"name\": \"Ana Garcia\",\n        \"title\": \"Senior Developer\",\n        \"department\": \"Engineering\",\n        \"risk_score\": 0.35,\n        \"is_privileged\": False,\n        \"manager\": \"mchen@company.com\",\n    },\n    {\n        \"user_id\": \"cjohnson@company.com\",\n        \"name\": \"Chris Johnson\",\n        \"title\": \"CEO\",\n        \"department\": \"Executive\",\n        \"risk_score\": 0.95,\n        \"is_privileged\": True,\n        \"manager\": None,\n    },\n    {\n        \"user_id\": \"blee@company.com\",\n        \"name\": \"Bob Lee\",\n        \"title\": \"SOC Analyst\",\n        \"department\": \"Security\",\n        \"risk_score\": 0.25,\n        \"is_privileged\": False,\n        \"manager\": \"ciso@company.com\",\n    },\n]\n\nfor user in users:\n    db.collection(\"users\").document(user[\"user_id\"]).set(user)\n    print(f\"  \u2713 Created user: {user['name']}\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(users)} users\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 14: Create Firestore Collections - Playbooks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates SOC response playbooks.\n\nprint(\"Creating Firestore playbooks collection...\")\nprint(\"-\" * 50)\n\nplaybooks = [\n    {\n        \"playbook_id\": \"PB-LOGIN-001\",\n        \"name\": \"Anomalous Login Response\",\n        \"description\": \"Response procedure for anomalous login alerts\",\n        \"alert_types\": [\"anomalous_login\"],\n        \"steps\": [\n            \"Check user travel status\",\n            \"Verify VPN location\",\n            \"Confirm MFA completion\",\n            \"Check device fingerprint\",\n            \"Review historical patterns\"\n        ],\n        \"auto_actions\": [\"false_positive_close\", \"enrich_and_wait\"],\n        \"escalation_actions\": [\"escalate_tier2\", \"escalate_incident\"],\n        \"sla_minutes\": 15,\n        \"owner\": \"soc_management@company.com\",\n    },\n    {\n        \"playbook_id\": \"PB-PHISH-001\",\n        \"name\": \"Phishing Response\",\n        \"description\": \"Response procedure for phishing alerts\",\n        \"alert_types\": [\"phishing\"],\n        \"steps\": [\n            \"Check sender reputation\",\n            \"Analyze URLs\",\n            \"Check for known campaigns\",\n            \"Quarantine if malicious\",\n            \"Notify affected users\"\n        ],\n        \"auto_actions\": [\"auto_remediate\"],\n        \"escalation_actions\": [\"escalate_tier2\"],\n        \"sla_minutes\": 30,\n        \"owner\": \"soc_management@company.com\",\n    },\n    {\n        \"playbook_id\": \"PB-MALWARE-001\",\n        \"name\": \"Malware Response\",\n        \"description\": \"Response procedure for malware detection\",\n        \"alert_types\": [\"malware_detection\"],\n        \"steps\": [\n            \"Assess asset criticality\",\n            \"Check for lateral movement\",\n            \"Isolate if confirmed\",\n            \"Collect forensic data\",\n            \"Initiate IR if critical\"\n        ],\n        \"auto_actions\": [\"auto_remediate\"],\n        \"escalation_actions\": [\"escalate_incident\"],\n        \"sla_minutes\": 10,\n        \"owner\": \"ir_team@company.com\",\n    },\n    {\n        \"playbook_id\": \"PB-DLP-001\",\n        \"name\": \"Data Exfiltration Response\",\n        \"description\": \"Response procedure for DLP alerts\",\n        \"alert_types\": [\"data_exfiltration\"],\n        \"steps\": [\n            \"Identify data classification\",\n            \"Assess volume and destination\",\n            \"Check user authorization\",\n            \"Block if unauthorized\",\n            \"Escalate to legal if PII\"\n        ],\n        \"auto_actions\": [],\n        \"escalation_actions\": [\"escalate_incident\"],\n        \"sla_minutes\": 5,\n        \"owner\": \"ir_team@company.com\",\n    },\n]\n\nfor playbook in playbooks:\n    db.collection(\"playbooks\").document(playbook[\"playbook_id\"]).set(playbook)\n    print(f\"  \u2713 Created playbook: {playbook['name']}\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(playbooks)} playbooks\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 15: Create Firestore Collections - Attack Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates learned attack patterns (the key to compounding).\n\nprint(\"Creating Firestore patterns collection...\")\nprint(\"-\" * 50)\n\npatterns = [\n    {\n        \"pattern_id\": \"PAT-TRAVEL-001\",\n        \"name\": \"Travel Login False Positive\",\n        \"description\": \"Login from travel destination with matching VPN and MFA\",\n        \"alert_types\": [\"anomalous_login\"],\n        \"match_criteria\": {\n            \"user_traveling\": True,\n            \"vpn_matches_location\": True,\n            \"mfa_completed\": True\n        },\n        \"fp_rate\": 0.94,\n        \"occurrence_count\": 127,\n        \"confidence\": 0.92,\n        \"recommended_action\": \"false_positive_close\",\n        \"created_at\": \"2025-10-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-30T00:00:00Z\",\n    },\n    {\n        \"pattern_id\": \"PAT-PHISH-KNOWN\",\n        \"name\": \"Known Phishing Campaign\",\n        \"description\": \"Email matches known phishing campaign signature\",\n        \"alert_types\": [\"phishing\"],\n        \"match_criteria\": {\n            \"known_campaign_signature\": True\n        },\n        \"fp_rate\": 0.02,\n        \"occurrence_count\": 89,\n        \"confidence\": 0.96,\n        \"recommended_action\": \"auto_remediate\",\n        \"created_at\": \"2025-11-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-28T00:00:00Z\",\n    },\n    {\n        \"pattern_id\": \"PAT-MALWARE-ISOLATE\",\n        \"name\": \"Standard Malware Isolation\",\n        \"description\": \"Non-critical asset with confirmed malware hash\",\n        \"alert_types\": [\"malware_detection\"],\n        \"match_criteria\": {\n            \"asset_criticality_not\": \"critical\",\n            \"hash_confirmed\": True\n        },\n        \"fp_rate\": 0.08,\n        \"occurrence_count\": 34,\n        \"confidence\": 0.91,\n        \"recommended_action\": \"auto_remediate\",\n        \"created_at\": \"2025-09-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-25T00:00:00Z\",\n    },\n    {\n        \"pattern_id\": \"PAT-VPN-KNOWN\",\n        \"name\": \"Known VPN Provider\",\n        \"description\": \"Login from recognized corporate VPN provider\",\n        \"alert_types\": [\"anomalous_login\"],\n        \"match_criteria\": {\n            \"vpn_provider_known\": True,\n            \"device_fingerprint_match\": True\n        },\n        \"fp_rate\": 0.96,\n        \"occurrence_count\": 245,\n        \"confidence\": 0.94,\n        \"recommended_action\": \"false_positive_close\",\n        \"created_at\": \"2025-08-15T00:00:00Z\",\n        \"updated_at\": \"2026-01-29T00:00:00Z\",\n    },\n    {\n        \"pattern_id\": \"PAT-LOGIN-NORMAL\",\n        \"name\": \"Normal Location Login\",\n        \"description\": \"Login from user's normal location during business hours\",\n        \"alert_types\": [\"anomalous_login\"],\n        \"match_criteria\": {\n            \"normal_location\": True,\n            \"business_hours\": True\n        },\n        \"fp_rate\": 0.98,\n        \"occurrence_count\": 2847,\n        \"confidence\": 0.97,\n        \"recommended_action\": \"false_positive_close\",\n        \"created_at\": \"2025-06-01T00:00:00Z\",\n        \"updated_at\": \"2026-01-30T00:00:00Z\",\n    },\n]\n\nfor pattern in patterns:\n    db.collection(\"patterns\").document(pattern[\"pattern_id\"]).set(pattern)\n    print(f\"  \u2713 Created pattern: {pattern['name']}\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(patterns)} attack patterns\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 16: Create Firestore Collections - Alerts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates sample alerts for the demo.\n\nprint(\"Creating Firestore alerts collection...\")\nprint(\"-\" * 50)\n\nalerts = [\n    {\n        \"alert_id\": \"ALERT-7823\",\n        \"alert_type\": \"anomalous_login\",\n        \"severity\": \"medium\",\n        \"source\": \"Splunk\",\n        \"source_rule_id\": \"anomalous_login_v2\",\n        \"asset_id\": \"LAPTOP-JSMITH\",\n        \"asset_hostname\": \"LAPTOP-JSMITH\",\n        \"user_id\": \"jsmith@company.com\",\n        \"source_location\": \"Singapore\",\n        \"source_ip\": \"103.45.67.89\",\n        \"timestamp\": \"2026-01-31T03:47:00Z\",\n        \"status\": \"pending\",\n        \"description\": \"Login from unusual location: Singapore at 3:47 AM local time\",\n    },\n    {\n        \"alert_id\": \"ALERT-7822\",\n        \"alert_type\": \"phishing\",\n        \"severity\": \"high\",\n        \"source\": \"Proofpoint\",\n        \"source_rule_id\": \"phishing_email_v3\",\n        \"asset_id\": \"MAIL-GW-01\",\n        \"asset_hostname\": \"MAIL-GW-01\",\n        \"user_id\": \"finance_team@company.com\",\n        \"source_location\": \"External\",\n        \"source_ip\": \"185.234.72.1\",\n        \"timestamp\": \"2026-01-31T08:15:00Z\",\n        \"status\": \"pending\",\n        \"description\": \"Phishing email detected targeting Finance team\",\n    },\n    {\n        \"alert_id\": \"ALERT-7821\",\n        \"alert_type\": \"malware_detection\",\n        \"severity\": \"critical\",\n        \"source\": \"CrowdStrike\",\n        \"source_rule_id\": \"malware_hash_match\",\n        \"asset_id\": \"SRV-DB-PROD-01\",\n        \"asset_hostname\": \"SRV-DB-PROD-01\",\n        \"user_id\": \"system\",\n        \"source_location\": \"Internal\",\n        \"source_ip\": \"10.0.1.50\",\n        \"timestamp\": \"2026-01-31T09:30:00Z\",\n        \"status\": \"pending\",\n        \"description\": \"Known malware hash detected on production database server\",\n    },\n    {\n        \"alert_id\": \"ALERT-7820\",\n        \"alert_type\": \"data_exfiltration\",\n        \"severity\": \"high\",\n        \"source\": \"DLP\",\n        \"source_rule_id\": \"dlp_sensitive_data\",\n        \"asset_id\": \"LAPTOP-MCHEN\",\n        \"asset_hostname\": \"LAPTOP-MCHEN\",\n        \"user_id\": \"mchen@company.com\",\n        \"source_location\": \"Internal\",\n        \"source_ip\": \"10.0.5.23\",\n        \"timestamp\": \"2026-01-31T10:15:00Z\",\n        \"status\": \"pending\",\n        \"dlp_classification\": \"confidential\",\n        \"description\": \"Large data transfer to external cloud storage\",\n    },\n    {\n        \"alert_id\": \"ALERT-7819\",\n        \"alert_type\": \"anomalous_login\",\n        \"severity\": \"low\",\n        \"source\": \"Splunk\",\n        \"source_rule_id\": \"anomalous_login_v2\",\n        \"asset_id\": \"LAPTOP-AGARCIA\",\n        \"asset_hostname\": \"LAPTOP-AGARCIA\",\n        \"user_id\": \"agarcia@company.com\",\n        \"source_location\": \"Denver\",\n        \"source_ip\": \"72.45.123.89\",\n        \"timestamp\": \"2026-01-31T07:30:00Z\",\n        \"status\": \"pending\",\n        \"description\": \"Login from Denver (user normally in San Francisco)\",\n    },\n]\n\nfor alert in alerts:\n    db.collection(\"alerts\").document(alert[\"alert_id\"]).set(alert)\n    print(f\"  \u2713 Created alert: {alert['alert_id']} ({alert['alert_type']})\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(alerts)} alerts\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 17: Create Firestore Collections - Deployments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the agent deployment registry (for Tab 2).\n\nprint(\"Creating Firestore deployments collection...\")\nprint(\"-\" * 50)\n\ndeployments = [\n    {\n        \"deployment_id\": \"soc-copilot-v3.1\",\n        \"agent_name\": \"soc-copilot\",\n        \"version\": \"v3.1\",\n        \"status\": \"active\",\n        \"traffic_pct\": 90,\n        \"deployed_at\": \"2026-01-15T00:00:00Z\",\n        \"config\": {\n            \"auto_close_threshold\": 0.88,\n            \"escalation_threshold\": 0.75,\n            \"model\": \"rule-based-v3\"\n        },\n        \"metrics_7d\": {\n            \"alerts_processed\": 4847,\n            \"auto_close_rate\": 0.732,\n            \"avg_confidence\": 0.89,\n            \"escalation_rate\": 0.12,\n        }\n    },\n    {\n        \"deployment_id\": \"soc-copilot-v3.2\",\n        \"agent_name\": \"soc-copilot\",\n        \"version\": \"v3.2\",\n        \"status\": \"canary\",\n        \"traffic_pct\": 10,\n        \"deployed_at\": \"2026-01-28T00:00:00Z\",\n        \"config\": {\n            \"auto_close_threshold\": 0.90,\n            \"escalation_threshold\": 0.78,\n            \"model\": \"rule-based-v3.2\"\n        },\n        \"metrics_7d\": {\n            \"alerts_processed\": 253,\n            \"auto_close_rate\": 0.718,\n            \"avg_confidence\": 0.87,\n            \"escalation_rate\": 0.15,\n        }\n    },\n]\n\nfor deployment in deployments:\n    db.collection(\"deployments\").document(deployment[\"deployment_id\"]).set(deployment)\n    print(f\"  \u2713 Created deployment: {deployment['version']} ({deployment['status']})\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(deployments)} deployments\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 18: Create Firestore Collections - Travel Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates travel records for context enrichment.\n\nprint(\"Creating Firestore travel collection...\")\nprint(\"-\" * 50)\n\ntravel_records = [\n    {\n        \"travel_id\": \"TRAVEL-001\",\n        \"user_id\": \"jsmith@company.com\",\n        \"destination\": \"Singapore\",\n        \"start_date\": \"2026-01-28\",\n        \"end_date\": \"2026-02-05\",\n        \"purpose\": \"Client meetings\",\n        \"vpn_expected\": [\"Marriott Hotels\", \"Singapore Airlines Lounge\"],\n        \"approved_by\": \"cjohnson@company.com\",\n    },\n    {\n        \"travel_id\": \"TRAVEL-002\",\n        \"user_id\": \"agarcia@company.com\",\n        \"destination\": \"Denver\",\n        \"start_date\": \"2026-01-30\",\n        \"end_date\": \"2026-02-01\",\n        \"purpose\": \"Team offsite\",\n        \"vpn_expected\": [\"Hilton Hotels\"],\n        \"approved_by\": \"mchen@company.com\",\n    },\n]\n\nfor travel in travel_records:\n    db.collection(\"travel\").document(travel[\"travel_id\"]).set(travel)\n    print(f\"  \u2713 Created travel: {travel['user_id']} \u2192 {travel['destination']}\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(travel_records)} travel records\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 19: Create Firestore Collections - Decisions (Sample)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates sample decision records.\n\nprint(\"Creating Firestore decisions collection...\")\nprint(\"-\" * 50)\n\ndecisions = [\n    {\n        \"decision_id\": \"DEC-7818\",\n        \"alert_id\": \"ALERT-7818\",\n        \"action\": \"false_positive_close\",\n        \"confidence\": 0.94,\n        \"pattern_id\": \"PAT-VPN-KNOWN\",\n        \"reasoning\": \"User connected via known corporate VPN. Device fingerprint matched.\",\n        \"eval_gate_passed\": True,\n        \"executed_at\": \"2026-01-30T14:30:00Z\",\n        \"created_by\": \"soc-copilot-v3.1\",\n    },\n    {\n        \"decision_id\": \"DEC-7817\",\n        \"alert_id\": \"ALERT-7817\",\n        \"action\": \"auto_remediate\",\n        \"confidence\": 0.96,\n        \"pattern_id\": \"PAT-PHISH-KNOWN\",\n        \"reasoning\": \"Email matched known Q4 phishing campaign signature. Auto-quarantined.\",\n        \"eval_gate_passed\": True,\n        \"executed_at\": \"2026-01-30T11:15:00Z\",\n        \"created_by\": \"soc-copilot-v3.1\",\n    },\n]\n\nfor decision in decisions:\n    db.collection(\"decisions\").document(decision[\"decision_id\"]).set(decision)\n    print(f\"  \u2713 Created decision: {decision['decision_id']} ({decision['action']})\")\n\nprint(\"-\" * 50)\nprint(f\"\u2713 Created {len(decisions)} sample decisions\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 20: Test Vertex AI (Gemini) Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This verifies that Gemini is working for LLM narration.\n\nprint(\"Testing Vertex AI (Gemini) connection...\")\nprint(\"-\" * 50)\n\nfrom vertexai.generative_models import GenerativeModel\n\ntry:\n    model = GenerativeModel(\"gemini-2.0-flash-001\")\n    response = model.generate_content(\n        \"Say 'SOC Copilot ready' in exactly 3 words.\",\n        generation_config={\n            \"temperature\": 0.1,\n            \"max_output_tokens\": 20,\n        }\n    )\n    print(f\"\u2713 Gemini response: {response.text.strip()}\")\n    print(\"\u2713 Vertex AI connection successful\")\nexcept Exception as e:\n    print(f\"\u2717 Error: {e}\")\n    print(\"  Make sure Vertex AI API is enabled and you have quota\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 21: Configure Neo4j Aura Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This cell tries to load Neo4j credentials from Colab Secrets first.\n# If not found, it falls back to the manual values below.\n#\n# TO USE COLAB SECRETS (Recommended - keeps passwords out of code):\n# 1. Click the \ud83d\udd11 key icon in the left sidebar\n# 2. Add these secrets:\n#    - NEO4J_URI: your Neo4j Aura URI (e.g., \"neo4j+s://a118b6f6.databases.neo4j.io\")\n#    - NEO4J_PASSWORD: your Neo4j password\n#    - ANTHROPIC_API_KEY: your Anthropic API key (for later use)\n#\n# OR: Just edit the fallback values below.\n\n# Try to load from Colab Secrets first\ntry:\n    from google.colab import userdata\n    NEO4J_URI = userdata.get('NEO4J_URI')\n    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n    NEO4J_USER = \"neo4j\"  # Usually always 'neo4j' for Aura\n    print(\"\u2713 Loaded Neo4j credentials from Colab Secrets\")\nexcept (ImportError, Exception) as e:\n    # Fallback to manual configuration\n    NEO4J_URI = \"neo4j+s://xxxxxxxx.databases.neo4j.io\"  # \u2190 CHANGE THIS\n    NEO4J_USER = \"neo4j\"\n    NEO4J_PASSWORD = \"your-password-here\"  # \u2190 CHANGE THIS\n    print(\"\u2139 Using manual Neo4j configuration\")\n\nprint(\"Neo4j configuration set:\")\nprint(f\"  URI: {NEO4J_URI}\")\nprint(f\"  User: {NEO4J_USER}\")\nprint(f\"  Password: {'*' * len(NEO4J_PASSWORD)}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 22: Test Neo4j Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This verifies the Neo4j connection works.\n\nfrom neo4j import GraphDatabase\n\nprint(\"Testing Neo4j connection...\")\nprint(\"-\" * 50)\n\ntry:\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    with driver.session() as session:\n        result = session.run(\"RETURN 'Connection successful!' AS message\")\n        record = result.single()\n        print(f\"\u2713 {record['message']}\")\n    driver.close()\n    print(\"\u2713 Neo4j connection verified\")\nexcept Exception as e:\n    print(f\"\u2717 Connection failed: {e}\")\n    print(\"  Check your NEO4J_URI and NEO4J_PASSWORD\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 23: Clear Existing Neo4j Data (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Run this if you need to reset the Neo4j database.\n# WARNING: This deletes all existing nodes and relationships!\n\nCLEAR_DATABASE = False  # Change to True to clear\n\nif CLEAR_DATABASE:\n    print(\"Clearing existing Neo4j data...\")\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    with driver.session() as session:\n        session.run(\"MATCH (n) DETACH DELETE n\")\n    driver.close()\n    print(\"\u2713 Database cleared\")\nelse:\n    print(\"Skipping database clear (CLEAR_DATABASE = False)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 24: Create Neo4j Security Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the core security entity nodes in Neo4j.\n\nprint(\"Creating Neo4j security entity nodes...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # Create Assets\n    for asset in assets:\n        session.run(\"\"\"\n            MERGE (a:Asset {id: $id})\n            SET a.hostname = $hostname,\n                a.type = $type,\n                a.os = $os,\n                a.criticality = $criticality,\n                a.business_unit = $business_unit,\n                a.owner_id = $owner_id\n        \"\"\", id=asset[\"asset_id\"], hostname=asset[\"hostname\"], type=asset[\"type\"],\n            os=asset[\"os\"], criticality=asset[\"criticality\"], \n            business_unit=asset[\"business_unit\"], owner_id=asset[\"owner_id\"])\n    print(f\"  \u2713 Created {len(assets)} Asset nodes\")\n    \n    # Create Users\n    for user in users:\n        session.run(\"\"\"\n            MERGE (u:User {id: $id})\n            SET u.name = $name,\n                u.title = $title,\n                u.department = $department,\n                u.risk_score = $risk_score,\n                u.is_privileged = $is_privileged\n        \"\"\", id=user[\"user_id\"], name=user[\"name\"], title=user[\"title\"],\n            department=user[\"department\"], risk_score=user[\"risk_score\"],\n            is_privileged=user[\"is_privileged\"])\n    print(f\"  \u2713 Created {len(users)} User nodes\")\n    \n    # Create AlertTypes\n    alert_types = [\n        {\"id\": \"anomalous_login\", \"name\": \"Anomalous Login\", \"severity\": \"medium\", \"mitre\": \"T1078\"},\n        {\"id\": \"phishing\", \"name\": \"Phishing\", \"severity\": \"high\", \"mitre\": \"T1566\"},\n        {\"id\": \"malware_detection\", \"name\": \"Malware Detection\", \"severity\": \"critical\", \"mitre\": \"T1204\"},\n        {\"id\": \"data_exfiltration\", \"name\": \"Data Exfiltration\", \"severity\": \"high\", \"mitre\": \"T1048\"},\n    ]\n    for at in alert_types:\n        session.run(\"\"\"\n            MERGE (at:AlertType {id: $id})\n            SET at.name = $name,\n                at.severity = $severity,\n                at.mitre_technique = $mitre\n        \"\"\", **at)\n    print(f\"  \u2713 Created {len(alert_types)} AlertType nodes\")\n    \n    # Create Attack Patterns\n    for pattern in patterns:\n        session.run(\"\"\"\n            MERGE (p:AttackPattern {id: $id})\n            SET p.name = $name,\n                p.fp_rate = $fp_rate,\n                p.occurrence_count = $occurrence_count,\n                p.confidence = $confidence\n        \"\"\", id=pattern[\"pattern_id\"], name=pattern[\"name\"],\n            fp_rate=pattern[\"fp_rate\"], occurrence_count=pattern[\"occurrence_count\"],\n            confidence=pattern[\"confidence\"])\n    print(f\"  \u2713 Created {len(patterns)} AttackPattern nodes\")\n    \n    # Create Playbooks\n    for pb in playbooks:\n        session.run(\"\"\"\n            MERGE (pb:Playbook {id: $id})\n            SET pb.name = $name,\n                pb.sla_minutes = $sla_minutes\n        \"\"\", id=pb[\"playbook_id\"], name=pb[\"name\"], sla_minutes=pb[\"sla_minutes\"])\n    print(f\"  \u2713 Created {len(playbooks)} Playbook nodes\")\n    \n    # Create SLAs\n    slas = [\n        {\"id\": \"SLA-CRITICAL\", \"name\": \"Critical SLA\", \"response_time_minutes\": 10, \"severity\": \"critical\"},\n        {\"id\": \"SLA-HIGH\", \"name\": \"High SLA\", \"response_time_minutes\": 30, \"severity\": \"high\"},\n        {\"id\": \"SLA-MEDIUM\", \"name\": \"Medium SLA\", \"response_time_minutes\": 60, \"severity\": \"medium\"},\n    ]\n    for sla in slas:\n        session.run(\"\"\"\n            MERGE (s:SLA {id: $id})\n            SET s.name = $name,\n                s.response_time_minutes = $response_time_minutes,\n                s.severity = $severity\n        \"\"\", **sla)\n    print(f\"  \u2713 Created {len(slas)} SLA nodes\")\n    \n    # Create Travel Context\n    for travel in travel_records:\n        session.run(\"\"\"\n            MERGE (t:TravelContext {id: $id})\n            SET t.user_id = $user_id,\n                t.destination = $destination,\n                t.start_date = date($start_date),\n                t.end_date = date($end_date)\n        \"\"\", id=travel[\"travel_id\"], user_id=travel[\"user_id\"],\n            destination=travel[\"destination\"], start_date=travel[\"start_date\"],\n            end_date=travel[\"end_date\"])\n    print(f\"  \u2713 Created {len(travel_records)} TravelContext nodes\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Security entity nodes created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 25: Create Neo4j Security Relationships"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the relationships between security entities.\n\nprint(\"Creating Neo4j security relationships...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # User -[:ASSIGNED_TO]-> Asset\n    user_asset_mappings = [\n        (\"jsmith@company.com\", \"LAPTOP-JSMITH\"),\n        (\"mchen@company.com\", \"LAPTOP-MCHEN\"),\n        (\"agarcia@company.com\", \"LAPTOP-AGARCIA\"),\n    ]\n    for user_id, asset_id in user_asset_mappings:\n        session.run(\"\"\"\n            MATCH (u:User {id: $user_id})\n            MATCH (a:Asset {id: $asset_id})\n            MERGE (u)-[:ASSIGNED_TO]->(a)\n        \"\"\", user_id=user_id, asset_id=asset_id)\n    print(f\"  \u2713 Created ASSIGNED_TO relationships\")\n    \n    # User -[:HAS_TRAVEL]-> TravelContext\n    for travel in travel_records:\n        session.run(\"\"\"\n            MATCH (u:User {id: $user_id})\n            MATCH (t:TravelContext {id: $travel_id})\n            MERGE (u)-[:HAS_TRAVEL]->(t)\n        \"\"\", user_id=travel[\"user_id\"], travel_id=travel[\"travel_id\"])\n    print(f\"  \u2713 Created HAS_TRAVEL relationships\")\n    \n    # AlertType -[:HANDLED_BY]-> Playbook\n    alert_playbook_mappings = [\n        (\"anomalous_login\", \"PB-LOGIN-001\"),\n        (\"phishing\", \"PB-PHISH-001\"),\n        (\"malware_detection\", \"PB-MALWARE-001\"),\n        (\"data_exfiltration\", \"PB-DLP-001\"),\n    ]\n    for alert_type, playbook_id in alert_playbook_mappings:\n        session.run(\"\"\"\n            MATCH (at:AlertType {id: $alert_type})\n            MATCH (pb:Playbook {id: $playbook_id})\n            MERGE (at)-[:HANDLED_BY]->(pb)\n        \"\"\", alert_type=alert_type, playbook_id=playbook_id)\n    print(f\"  \u2713 Created HANDLED_BY relationships\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Security relationships created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 25B: Create Additional Relationships (CRITICAL FOR DEMO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# These relationships are required for the demo to work correctly:\n# - Asset -[:SUBJECT_TO]-> SLA (for SLA compliance checks)\n# - Alert -[:MATCHES]-> AttackPattern (for pattern matching)\n\nprint(\"Creating additional relationships (critical for demo)...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # Asset -[:SUBJECT_TO]-> SLA (based on asset criticality)\n    asset_sla_mappings = [\n        (\"LAPTOP-JSMITH\", \"SLA-HIGH\"),      # High criticality asset\n        (\"SRV-DB-PROD\", \"SLA-CRITICAL\"),    # Critical server\n        (\"LAPTOP-MCHEN\", \"SLA-HIGH\"),       # High criticality asset\n        (\"LAPTOP-AGARCIA\", \"SLA-MEDIUM\"),   # Medium criticality asset\n        (\"MAIL-GW\", \"SLA-CRITICAL\"),        # Critical infrastructure\n    ]\n    for asset_id, sla_id in asset_sla_mappings:\n        session.run(\"\"\"\n            MATCH (a:Asset {id: $asset_id})\n            MATCH (s:SLA {id: $sla_id})\n            MERGE (a)-[:SUBJECT_TO]->(s)\n        \"\"\", asset_id=asset_id, sla_id=sla_id)\n    print(f\"  \u2713 Created {len(asset_sla_mappings)} SUBJECT_TO relationships\")\n    \n    # Alert -[:MATCHES]-> AttackPattern (for known pattern matching)\n    alert_pattern_mappings = [\n        (\"ALERT-7823\", \"PAT-TRAVEL-001\"),   # John Smith travel - matches travel pattern\n        (\"ALERT-7822\", \"PAT-MALWARE-ISOLATE\"),  # Malware detection\n        (\"ALERT-7821\", \"PAT-PHISH-KNOWN\"),  # Phishing email\n        (\"ALERT-7820\", \"PAT-VPN-KNOWN\"),    # VPN login\n        (\"ALERT-7819\", \"PAT-LOGIN-NORMAL\"), # Normal login pattern\n    ]\n    for alert_id, pattern_id in alert_pattern_mappings:\n        session.run(\"\"\"\n            MATCH (alert:Alert {id: $alert_id})\n            MATCH (p:AttackPattern {id: $pattern_id})\n            MERGE (alert)-[:MATCHES]->(p)\n        \"\"\", alert_id=alert_id, pattern_id=pattern_id)\n    print(f\"  \u2713 Created {len(alert_pattern_mappings)} MATCHES relationships\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Additional relationships created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 26: Create Neo4j Alert Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates alert nodes and their relationships.\n\nprint(\"Creating Neo4j alert nodes and relationships...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    for alert in alerts:\n        # Create Alert node\n        session.run(\"\"\"\n            MERGE (a:Alert {id: $id})\n            SET a.alert_type = $alert_type,\n                a.severity = $severity,\n                a.source = $source,\n                a.source_location = $source_location,\n                a.timestamp = datetime($timestamp),\n                a.status = $status,\n                a.description = $description\n        \"\"\", id=alert[\"alert_id\"], alert_type=alert[\"alert_type\"],\n            severity=alert[\"severity\"], source=alert[\"source\"],\n            source_location=alert[\"source_location\"], timestamp=alert[\"timestamp\"],\n            status=alert[\"status\"], description=alert[\"description\"])\n        \n        # Alert -[:DETECTED_ON]-> Asset\n        session.run(\"\"\"\n            MATCH (alert:Alert {id: $alert_id})\n            MATCH (asset:Asset {id: $asset_id})\n            MERGE (alert)-[:DETECTED_ON]->(asset)\n        \"\"\", alert_id=alert[\"alert_id\"], asset_id=alert[\"asset_id\"])\n        \n        # Alert -[:INVOLVES]-> User\n        session.run(\"\"\"\n            MATCH (alert:Alert {id: $alert_id})\n            MATCH (user:User {id: $user_id})\n            MERGE (alert)-[:INVOLVES]->(user)\n        \"\"\", alert_id=alert[\"alert_id\"], user_id=alert[\"user_id\"])\n        \n        # Alert -[:CLASSIFIED_AS]-> AlertType\n        session.run(\"\"\"\n            MATCH (alert:Alert {id: $alert_id})\n            MATCH (at:AlertType {id: $alert_type})\n            MERGE (alert)-[:CLASSIFIED_AS]->(at)\n        \"\"\", alert_id=alert[\"alert_id\"], alert_type=alert[\"alert_type\"])\n        \n    print(f\"  \u2713 Created {len(alerts)} Alert nodes with relationships\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Alert nodes and relationships created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 27: Create Decision Trace Nodes (THE KEY DIFFERENTIATOR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the Decision Trace schema that makes our demo unique.\n# - Decision nodes (5)\n# - DecisionContext nodes (5)\n# - EvolutionEvent nodes (3)\n\nprint(\"Creating Decision Trace nodes (THE KEY DIFFERENTIATOR)...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # Create Decision nodes\n    decision_nodes = [\n        {\n            \"id\": \"DEC-7823\",\n            \"type\": \"alert_triage\",\n            \"reasoning\": \"User has active travel to Singapore. VPN from Marriott Hotels matches known provider. MFA completed. Pattern PAT-TRAVEL-001 matched with 127 similar cases.\",\n            \"confidence\": 0.92,\n            \"timestamp\": \"2026-01-31T03:48:00Z\",\n            \"alert_id\": \"ALERT-7823\",\n            \"action_taken\": \"false_positive_close\"\n        },\n        {\n            \"id\": \"DEC-7822\",\n            \"type\": \"alert_triage\",\n            \"reasoning\": \"Email matches known Q4 phishing campaign (CAMP-2026-Q1-003). URL analysis confirmed malicious. Auto-quarantine executed.\",\n            \"confidence\": 0.96,\n            \"timestamp\": \"2026-01-31T08:16:00Z\",\n            \"alert_id\": \"ALERT-7822\",\n            \"action_taken\": \"auto_remediate\"\n        },\n        {\n            \"id\": \"DEC-7821\",\n            \"type\": \"alert_triage\",\n            \"reasoning\": \"Malware detected on CRITICAL production database server. Immediate IR escalation required per playbook PB-MALWARE-001.\",\n            \"confidence\": 0.96,\n            \"timestamp\": \"2026-01-31T09:31:00Z\",\n            \"alert_id\": \"ALERT-7821\",\n            \"action_taken\": \"escalate_incident\"\n        },\n        {\n            \"id\": \"DEC-7820\",\n            \"type\": \"alert_triage\",\n            \"reasoning\": \"Potential data exfiltration from privileged user. Confidential data classification. Escalating to IR per policy.\",\n            \"confidence\": 0.97,\n            \"timestamp\": \"2026-01-31T10:16:00Z\",\n            \"alert_id\": \"ALERT-7820\",\n            \"action_taken\": \"escalate_incident\"\n        },\n        {\n            \"id\": \"DEC-7819\",\n            \"type\": \"alert_triage\",\n            \"reasoning\": \"User Ana Garcia has active travel to Denver. Login location matches travel destination. Low risk user.\",\n            \"confidence\": 0.91,\n            \"timestamp\": \"2026-01-31T07:31:00Z\",\n            \"alert_id\": \"ALERT-7819\",\n            \"action_taken\": \"false_positive_close\"\n        },\n    ]\n    \n    for dec in decision_nodes:\n        session.run(\"\"\"\n            MERGE (d:Decision {id: $id})\n            SET d.type = $type,\n                d.reasoning = $reasoning,\n                d.confidence = $confidence,\n                d.timestamp = datetime($timestamp),\n                d.alert_id = $alert_id,\n                d.action_taken = $action_taken\n        \"\"\", **dec)\n    print(f\"  \u2713 Created {len(decision_nodes)} Decision nodes\")\n    \n    # Create DecisionContext nodes\n    context_nodes = [\n        {\n            \"id\": \"CTX-7823\",\n            \"decision_id\": \"DEC-7823\",\n            \"user_snapshot\": '{\"name\": \"John Smith\", \"title\": \"VP Finance\", \"risk_score\": 0.85}',\n            \"asset_snapshot\": '{\"hostname\": \"LAPTOP-JSMITH\", \"criticality\": \"high\"}',\n            \"patterns_matched\": [\"PAT-TRAVEL-001\"],\n            \"nodes_consulted\": 47\n        },\n        {\n            \"id\": \"CTX-7822\",\n            \"decision_id\": \"DEC-7822\",\n            \"user_snapshot\": '{\"name\": \"Finance Team\", \"department\": \"Finance\"}',\n            \"asset_snapshot\": '{\"hostname\": \"MAIL-GW-01\", \"criticality\": \"critical\"}',\n            \"patterns_matched\": [\"PAT-PHISH-KNOWN\"],\n            \"nodes_consulted\": 23\n        },\n        {\n            \"id\": \"CTX-7821\",\n            \"decision_id\": \"DEC-7821\",\n            \"user_snapshot\": '{\"name\": \"system\", \"type\": \"service_account\"}',\n            \"asset_snapshot\": '{\"hostname\": \"SRV-DB-PROD-01\", \"criticality\": \"critical\"}',\n            \"patterns_matched\": [],\n            \"nodes_consulted\": 31\n        },\n        {\n            \"id\": \"CTX-7820\",\n            \"decision_id\": \"DEC-7820\",\n            \"user_snapshot\": '{\"name\": \"Mary Chen\", \"title\": \"Director Engineering\", \"risk_score\": 0.72}',\n            \"asset_snapshot\": '{\"hostname\": \"LAPTOP-MCHEN\", \"criticality\": \"high\"}',\n            \"patterns_matched\": [],\n            \"nodes_consulted\": 28\n        },\n        {\n            \"id\": \"CTX-7819\",\n            \"decision_id\": \"DEC-7819\",\n            \"user_snapshot\": '{\"name\": \"Ana Garcia\", \"title\": \"Senior Developer\", \"risk_score\": 0.35}',\n            \"asset_snapshot\": '{\"hostname\": \"LAPTOP-AGARCIA\", \"criticality\": \"medium\"}',\n            \"patterns_matched\": [\"PAT-TRAVEL-001\"],\n            \"nodes_consulted\": 35\n        },\n    ]\n    \n    for ctx in context_nodes:\n        session.run(\"\"\"\n            MERGE (c:DecisionContext {id: $id})\n            SET c.decision_id = $decision_id,\n                c.user_snapshot = $user_snapshot,\n                c.asset_snapshot = $asset_snapshot,\n                c.patterns_matched = $patterns_matched,\n                c.nodes_consulted = $nodes_consulted\n        \"\"\", **ctx)\n    print(f\"  \u2713 Created {len(context_nodes)} DecisionContext nodes\")\n    \n    # Create EvolutionEvent nodes\n    evolution_nodes = [\n        {\n            \"id\": \"EVO-0891\",\n            \"event_type\": \"pattern_confidence\",\n            \"triggered_by\": \"DEC-7823\",\n            \"before_state\": '{\"confidence\": 0.91, \"occurrence_count\": 126}',\n            \"after_state\": '{\"confidence\": 0.94, \"occurrence_count\": 127}',\n            \"description\": \"Pattern PAT-TRAVEL-001 confidence increased based on successful false positive close\",\n            \"timestamp\": \"2026-01-31T03:48:30Z\"\n        },\n        {\n            \"id\": \"EVO-0890\",\n            \"event_type\": \"threshold_adjustment\",\n            \"triggered_by\": \"DEC-7819\",\n            \"before_state\": '{\"auto_close_threshold\": 0.88}',\n            \"after_state\": '{\"auto_close_threshold\": 0.90}',\n            \"description\": \"Auto-close threshold for travel alerts adjusted based on sustained accuracy\",\n            \"timestamp\": \"2026-01-31T07:32:00Z\"\n        },\n        {\n            \"id\": \"EVO-0889\",\n            \"event_type\": \"new_pattern\",\n            \"triggered_by\": \"DEC-7822\",\n            \"before_state\": '{}',\n            \"after_state\": '{\"pattern_id\": \"PAT-PHISH-Q1-2026\", \"confidence\": 0.94}',\n            \"description\": \"New phishing campaign pattern identified from Q1 2026 campaign\",\n            \"timestamp\": \"2026-01-31T08:17:00Z\"\n        },\n    ]\n    \n    for evo in evolution_nodes:\n        session.run(\"\"\"\n            MERGE (e:EvolutionEvent {id: $id})\n            SET e.event_type = $event_type,\n                e.triggered_by = $triggered_by,\n                e.before_state = $before_state,\n                e.after_state = $after_state,\n                e.description = $description,\n                e.timestamp = datetime($timestamp)\n        \"\"\", **evo)\n    print(f\"  \u2713 Created {len(evolution_nodes)} EvolutionEvent nodes\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Decision Trace nodes created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 28: Create Decision Trace Relationships (INCLUDING TRIGGERED_EVOLUTION)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This creates the relationships that connect decision traces.\n# CRITICAL: This includes TRIGGERED_EVOLUTION - THE KEY DIFFERENTIATOR\n\nprint(\"Creating Decision Trace relationships...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # Decision -[:HAD_CONTEXT]-> DecisionContext\n    for ctx in context_nodes:\n        session.run(\"\"\"\n            MATCH (d:Decision {id: $decision_id})\n            MATCH (c:DecisionContext {id: $context_id})\n            MERGE (d)-[:HAD_CONTEXT]->(c)\n        \"\"\", decision_id=ctx[\"decision_id\"], context_id=ctx[\"id\"])\n    print(\"  \u2713 Created HAD_CONTEXT relationships\")\n    \n    # Decision -[:FOR_ALERT]-> Alert\n    for dec in decision_nodes:\n        session.run(\"\"\"\n            MATCH (d:Decision {id: $decision_id})\n            MATCH (a:Alert {id: $alert_id})\n            MERGE (d)-[:FOR_ALERT]->(a)\n        \"\"\", decision_id=dec[\"id\"], alert_id=dec[\"alert_id\"])\n    print(\"  \u2713 Created FOR_ALERT relationships\")\n    \n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    # TRIGGERED_EVOLUTION \u2014 THE KEY DIFFERENTIATOR\n    # This is what makes our demo unique. What SIEMs don't have.\n    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n    \n    triggered_evolutions = [\n        {\n            \"decision_id\": \"DEC-7823\",\n            \"evolution_id\": \"EVO-0891\",\n            \"impact\": \"pattern_confidence_increase\",\n            \"magnitude\": 0.03,\n        },\n        {\n            \"decision_id\": \"DEC-7819\",\n            \"evolution_id\": \"EVO-0890\",\n            \"impact\": \"threshold_adjustment\",\n            \"magnitude\": 0.02,\n        },\n        {\n            \"decision_id\": \"DEC-7822\",\n            \"evolution_id\": \"EVO-0889\",\n            \"impact\": \"new_pattern_created\",\n            \"magnitude\": 1.0,\n        },\n    ]\n    \n    for te in triggered_evolutions:\n        session.run(\"\"\"\n            MATCH (d:Decision {id: $decision_id})\n            MATCH (e:EvolutionEvent {id: $evolution_id})\n            MERGE (d)-[:TRIGGERED_EVOLUTION {\n                impact: $impact,\n                magnitude: $magnitude,\n                timestamp: datetime()\n            }]->(e)\n        \"\"\", **te)\n    print(f\"  \u2713 Created {len(triggered_evolutions)} TRIGGERED_EVOLUTION relationships (THE KEY)\")\n    \n    # CAUSED relationships (causal chain)\n    caused_relationships = [\n        {\n            \"from_decision\": \"DEC-7823\",\n            \"to_decision\": \"DEC-7819\",\n            \"reason\": \"Similar travel pattern recognition applied\"\n        },\n        {\n            \"from_decision\": \"DEC-7822\",\n            \"to_decision\": \"DEC-7817\",\n            \"reason\": \"Same phishing campaign signature\"\n        },\n    ]\n    \n    for caused in caused_relationships:\n        session.run(\"\"\"\n            MATCH (d1:Decision {id: $from_decision})\n            MATCH (d2:Decision {id: $to_decision})\n            MERGE (d1)-[:CAUSED {\n                reason: $reason,\n                timestamp: datetime()\n            }]->(d2)\n        \"\"\", **caused)\n    print(f\"  \u2713 Created {len(caused_relationships)} CAUSED relationships\")\n    \n    # PRECEDENT_FOR relationships\n    precedent_relationships = [\n        {\n            \"from_decision\": \"DEC-7823\",\n            \"to_decision\": \"DEC-7819\",\n            \"similarity\": 0.89,\n            \"cited_in_reasoning\": True\n        },\n        {\n            \"from_decision\": \"DEC-7822\",\n            \"to_decision\": \"DEC-7817\",\n            \"similarity\": 0.94,\n            \"cited_in_reasoning\": True\n        },\n    ]\n    \n    for prec in precedent_relationships:\n        session.run(\"\"\"\n            MATCH (d1:Decision {id: $from_decision})\n            MATCH (d2:Decision {id: $to_decision})\n            MERGE (d1)-[:PRECEDENT_FOR {\n                similarity: $similarity,\n                cited_in_reasoning: $cited_in_reasoning\n            }]->(d2)\n        \"\"\", **prec)\n    print(f\"  \u2713 Created {len(precedent_relationships)} PRECEDENT_FOR relationships\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Decision Trace relationships created\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 29: Verify Neo4j Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This verifies all the critical nodes and relationships are present.\n\nprint(\"Verifying Neo4j schema...\")\nprint(\"-\" * 50)\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n\nwith driver.session() as session:\n    # Count all node types\n    result = session.run(\"MATCH (n) RETURN labels(n)[0] as label, count(n) as count ORDER BY label\")\n    print(\"Node counts:\")\n    total_nodes = 0\n    for record in result:\n        print(f\"  {record['label']}: {record['count']}\")\n        total_nodes += record['count']\n    print(f\"  TOTAL: {total_nodes}\")\n    \n    print()\n    \n    # Count relationships\n    result = session.run(\"MATCH ()-[r]->() RETURN type(r) as type, count(r) as count ORDER BY type\")\n    print(\"Relationship counts:\")\n    total_rels = 0\n    for record in result:\n        print(f\"  {record['type']}: {record['count']}\")\n        total_rels += record['count']\n    print(f\"  TOTAL: {total_rels}\")\n    \n    print()\n    \n    # Verify TRIGGERED_EVOLUTION specifically\n    result = session.run(\"\"\"\n        MATCH (d:Decision)-[r:TRIGGERED_EVOLUTION]->(e:EvolutionEvent)\n        RETURN d.id as decision, e.id as evolution, r.impact as impact\n    \"\"\")\n    print(\"TRIGGERED_EVOLUTION relationships (THE KEY):\")\n    te_count = 0\n    for record in result:\n        print(f\"  {record['decision']} \u2192 {record['evolution']} ({record['impact']})\")\n        te_count += 1\n    \n    # Verification summary\n    print(\"-\" * 50)\n    expected_nodes = 46  # 5+5+4+5+4+3+2+5+5+5+3 (Asset+User+AlertType+Pattern+Playbook+SLA+Travel+Alert+Decision+Context+Evo)\n    expected_te = 3\n    \n    if te_count == expected_te:\n        print(f\"\u2713 TRIGGERED_EVOLUTION count correct: {te_count}\")\n    else:\n        print(f\"\u26a0 TRIGGERED_EVOLUTION count: {te_count} (expected {expected_te})\")\n    \n    print(f\"\u2713 Total nodes: {total_nodes}\")\n    print(f\"\u2713 Total relationships: {total_rels}\")\n\ndriver.close()\nprint(\"-\" * 50)\nprint(\"\u2713 Neo4j schema verified\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 30: Export Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This outputs the .env file content you'll need for local development.\n\nprint(\"=\" * 70)\nprint(\"ENVIRONMENT CONFIGURATION\")\nprint(\"Copy this to your .env file:\")\nprint(\"=\" * 70)\n\nenv_content = f\"\"\"\n# GCP Configuration\nPROJECT_ID={PROJECT_ID}\nREGION={REGION}\n\n# BigQuery\nBIGQUERY_DATASET={BIGQUERY_DATASET}\n\n# Firestore\nFIRESTORE_DATABASE=(default)\n\n# Neo4j Aura\nNEO4J_URI={NEO4J_URI}\nNEO4J_USER={NEO4J_USER}\nNEO4J_PASSWORD={NEO4J_PASSWORD}\n\n# Vertex AI\nVERTEX_AI_LOCATION={REGION}\n\"\"\"\n\nprint(env_content)\nprint(\"=\" * 70)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 31: Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This shows everything that was created.\n\nprint(\"=\" * 80)\nprint(\"                         SETUP COMPLETE!\")\nprint(\"=\" * 80)\nprint()\nprint(\"What was created:\")\nprint(\"-\" * 40)\nprint()\nprint(f\"BigQuery ({BIGQUERY_DATASET} dataset):\")\nprint(\"  * metric_contracts (10 SOC metrics with definitions)\")\nprint(\"  * soc_metrics (4 weeks of MTTR/MTTD/auto-close data)\")\nprint(\"  * detection_rules (5 rules with FP rates)\")\nprint(\"  * rule_sprawl_registry (1 duplicate rule for demo)\")\nprint()\nprint(\"Firestore (Native mode):\")\nprint(\"  * assets (5 devices/servers)\")\nprint(\"  * users (5 users with risk scores)\")\nprint(\"  * playbooks (4 response procedures)\")\nprint(\"  * patterns (5 learned attack patterns)\")\nprint(\"  * alerts (5 security alerts for demo)\")\nprint(\"  * deployments (2 agent versions: active + canary)\")\nprint(\"  * travel (2 travel records for context)\")\nprint(\"  * decisions (2 sample decisions)\")\nprint()\nprint(\"Neo4j Aura (security graph):\")\nprint(f\"  * ~{total_nodes} nodes (Assets, Users, Alerts, Patterns, etc.)\")\nprint(f\"  * ~{total_rels} relationships\")\nprint(\"  * Including: SUBJECT_TO (Asset\u2192SLA), MATCHES (Alert\u2192Pattern)\")\nprint()\nprint(\"Decision Trace Schema (THE KEY DIFFERENTIATOR):\")\nprint(\"  * 5 Decision nodes\")\nprint(\"  * 5 DecisionContext nodes\")\nprint(\"  * 3 EvolutionEvent nodes\")\nprint(\"  * 3 TRIGGERED_EVOLUTION relationships  <-- THE KEY ADDITION\")\nprint(\"  * 2 CAUSED relationships (causal chains)\")\nprint(\"  * 2 PRECEDENT_FOR relationships (precedent matching)\")\nprint()\nprint(\"=\" * 80)\nprint()\nprint(\"NEXT STEPS:\")\nprint(\"-\" * 40)\nprint(\"1. Copy the .env content above to your local project\")\nprint(\"2. Run: cd ~/projects/soc-copilot-demo\")\nprint(\"3. Run: claude\")\nprint(\"4. Ask Claude to start building Tab 2 (Runtime Evolution)\")\nprint()\nprint(\"Key file references:\")\nprint(\"  * CLAUDE.md \u2192 CLAUDE_ciso_v1.md\")\nprint(\"  * Build spec \u2192 vc_demo_build_spec_ciso_v1.md\")\nprint(\"  * Setup notebook \u2192 ciso_setup_notebook_v1.py (THIS FILE)\")\nprint()\nprint(\"=\" * 80)\nprint(\"                    SOC Copilot Demo Ready!\")\nprint(\"       Key differentiator: TRIGGERED_EVOLUTION relationships\")\nprint(\"=\" * 80)",
      "execution_count": null,
      "outputs": []
    }
  ]
}